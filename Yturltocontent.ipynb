{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afb85b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:13<00:00, 5.49MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import whisper\n",
    "\n",
    "from youtube_transcript_api import (\n",
    "    YouTubeTranscriptApi,\n",
    "    RequestBlocked,\n",
    "    NoTranscriptFound,\n",
    "    TranscriptsDisabled,\n",
    "    VideoUnavailable,\n",
    ")\n",
    "\n",
    "from typing import TypedDict, Optional\n",
    "\n",
    "class State(TypedDict):\n",
    "    url: str\n",
    "    user_query: str\n",
    "    transcript: Optional[str]\n",
    "    llama3_output: Optional[str]\n",
    "    mixtral_output: Optional[str]\n",
    "    final_output: Optional[str]\n",
    "\n",
    "\n",
    "# load Whisper model once (use \"tiny\" for speed)\n",
    "whisper_model = whisper.load_model(\"tiny\")  # or \"base\" if your PC is strong\n",
    "\n",
    "\n",
    "def _extract_video_id(url: str) -> str:\n",
    "    \"\"\"Handle normal YouTube URLs and youtu.be short links with params.\"\"\"\n",
    "    if \"v=\" in url:\n",
    "        return url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "    last_part = url.split(\"/\")[-1]\n",
    "    return last_part.split(\"?\")[0].split(\"&\")[0]\n",
    "\n",
    "\n",
    "def _transcribe_audio_locally(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads audio using yt-dlp and transcribes with local Whisper (offline).\n",
    "    Returns plain text.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        audio_path = os.path.join(tmpdir, \"audio.webm\")\n",
    "\n",
    "        cmd = [\n",
    "            \"yt-dlp\",\n",
    "            \"-x\",                   # extract audio\n",
    "            \"--audio-format\", \"webm\",\n",
    "            \"-o\", audio_path,\n",
    "            url,\n",
    "        ]\n",
    "\n",
    "        proc = subprocess.run(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "\n",
    "        if proc.returncode != 0:\n",
    "            print(\"[yt-dlp stdout]:\", proc.stdout)\n",
    "            print(\"[yt-dlp stderr]:\", proc.stderr)\n",
    "            raise RuntimeError(f\"yt-dlp failed with code {proc.returncode}\")\n",
    "\n",
    "        # Transcribe with Whisper (local)\n",
    "        result = whisper_model.transcribe(audio_path)\n",
    "        return result[\"text\"]\n",
    "\n",
    "\n",
    "def transcript_generator(state: State) -> State:\n",
    "    \"\"\"\n",
    "    1. Try YouTubeTranscriptApi (captions).\n",
    "    2. If blocked/unavailable → download audio + transcribe with Whisper.\n",
    "    3. Store final transcript in state[\"transcript\"] (or None if everything fails).\n",
    "    \"\"\"\n",
    "    url = state[\"url\"]\n",
    "    video_id = _extract_video_id(url)\n",
    "\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "    # ---- First try: captions API ----\n",
    "    try:\n",
    "        segments = ytt_api.fetch(video_id).to_raw_data()\n",
    "        transcript = \" \".join(seg[\"text\"] for seg in segments)\n",
    "        state[\"transcript\"] = transcript\n",
    "        return state\n",
    "    except (VideoUnavailable, RequestBlocked, NoTranscriptFound, TranscriptsDisabled) as e:\n",
    "        print(f\"[Caption API failed → Whisper fallback] {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Caption API unexpected error → Whisper fallback] {e}\")\n",
    "\n",
    "    # ---- Second try: audio + Whisper ----\n",
    "    try:\n",
    "        transcript = _transcribe_audio_locally(url)\n",
    "        state[\"transcript\"] = transcript\n",
    "    except Exception as e:\n",
    "        print(f\"[Whisper fallback failed] {e}\")\n",
    "        state[\"transcript\"] = None\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b5926c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[♪♪♪] ♪ We're no strangers to love ♪ ♪ You know the rules\n",
      "and so do I ♪ ♪ A full commitment's\n",
      "what I'm thinking of ♪ ♪ You wouldn't get this\n",
      "from any other guy ♪ ♪ I just wanna tell you\n",
      "how I'm feeling ♪ ♪ Gotta make you understand ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never g\n"
     ]
    }
   ],
   "source": [
    "state = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n",
    "    \n",
    "    \"user_query\": \"Generate notes\",\n",
    "    \"transcript\": None,\n",
    "    \"llama3_output\": None,\n",
    "    \"mixtral_output\": None,\n",
    "    \"final_output\": None,\n",
    "}\n",
    "\n",
    "result = transcript_generator(state)\n",
    "print(result[\"transcript\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f657369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[♪♪♪] ♪ We're no strangers to love ♪ ♪ You know the rules\n",
      "and so do I ♪ ♪ A full commitment's\n",
      "what I'm thinking of ♪ ♪ You wouldn't get this\n",
      "from any other guy ♪ ♪ I just wanna tell you\n",
      "how I'm feeling ♪ ♪ Gotta make you understand ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\n",
      "and desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\n",
      "and hurt you ♪ ♪ We've known each other\n",
      "for so long ♪ ♪ Your heart's been aching\n",
      "but you're too shy to say it ♪ ♪ Inside we both know\n",
      "what's been going ♪ ♪ We know the game\n",
      "and we're gonna play it ♪ ♪ And if you ask me\n",
      "how I'm feeling ♪ ♪ Don't tell me\n",
      "you're too blind to see ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\n",
      "and desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\n",
      "and hurt you ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\n",
      "and desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\n",
      "and hurt you ♪ ♪ (Ooh, give you up) ♪ ♪ (Ooh, give you up) ♪ ♪ Never gonna give,\n",
      "never gonna give ♪ ♪ (Give you up) ♪ ♪ Never gonna give,\n",
      "never gonna give ♪ ♪ (Give you up) ♪ ♪ We've known each other\n",
      "for so long ♪ ♪ Your heart's been aching\n",
      "but you're too shy to say it ♪ ♪ Inside we both know\n",
      "what's been going ♪ ♪ We know the game\n",
      "and we're gonna play it ♪ ♪ I just wanna tell you\n",
      "how I'm feeling ♪ ♪ Gotta make you understand ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\n",
      "and desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\n",
      "and hurt you ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\n",
      "and desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\n",
      "and hurt you ♪ ♪ Never gonna give you up ♪ ♪ Never gonna let you down ♪ ♪ Never gonna run around\n",
      "and desert you ♪ ♪ Never gonna make you cry ♪ ♪ Never gonna say goodbye ♪ ♪ Never gonna tell a lie\n",
      "and hurt you ♪\n"
     ]
    }
   ],
   "source": [
    "print(state['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94c5ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'GROQ_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[1;32m----> 6\u001b[0m groq_client \u001b[38;5;241m=\u001b[39m Groq(api_key\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGROQ_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mState\u001b[39;00m(TypedDict):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# input from user\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m                  \u001b[38;5;66;03m# YouTube URL\u001b[39;00m\n",
      "File \u001b[1;32md:\\langgraph\\langenv\\lib\\os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GROQ_API_KEY'"
     ]
    }
   ],
   "source": [
    "#Creating state schema for nodes\n",
    "from typing import TypedDict, Optional\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # reads the .env file\n",
    "\n",
    "from groq import Groq\n",
    "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "class State(TypedDict):\n",
    "    # input from user\n",
    "    url: str                  # YouTube URL\n",
    "    user_query: str           # what the user wants (notes, summary, etc.)\n",
    "\n",
    "    # intermediate data\n",
    "    transcript: Optional[str] # filled by transcript_generator\n",
    "    llama3_output: Optional[str]   # will be filled by LLaMA3 node\n",
    "    mixtral_output: Optional[str]  # will be filled by Mixtral node\n",
    "\n",
    "    # final result\n",
    "    final_output: Optional[str]    # final answer from Qwen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87754e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello guys. So welcome to this amazing crash course on building agentic AI application with the help of Langraph. This entire crash course has been divided into three important parts and each and every part will be somewhere around 2 to three hours of videos. Right. And here you can basically see what in which way we are going to cover all the topics and uh where we are going to aim once we reach to the part three. Okay. So in the part one you'll be able to see that we will be covering various f\n"
     ]
    }
   ],
   "source": [
    "state: State = {\n",
    "    \"url\": \"https://www.youtube.com/watch?v=dIb-DujRNEo&list=PLZoTAELRMXVPFd7JdvB-rnTb_5V26NYNO&index=5\",\n",
    "    \"user_query\": \"Generate notes\",\n",
    "    \"transcript\": None,\n",
    "    \"llama3_output\": None,\n",
    "    \"mixtral_output\": None,\n",
    "    \"final_output\": None,\n",
    "}\n",
    "\n",
    "state = transcript_generator(state)\n",
    "\n",
    "if state[\"transcript\"]:\n",
    "    print(state[\"transcript\"][:500])\n",
    "else:\n",
    "    print(\"Transcript failed (both captions and Whisper).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6395c7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello guys. So welcome to this amazing crash course on building agentic AI application with the help of Langraph. This entire crash course has been divided into three important parts and each and every part will be somewhere around 2 to three hours of videos. Right. And here you can basically see what in which way we are going to cover all the topics and uh where we are going to aim once we reach to the part three. Okay. So in the part one you'll be able to see that we will be covering various fundamental techniques which are really really important in order to build agentic AI application. some of the important topics like how to build a chatbot, how to integrate tools, how to integrate multiple tools in a chatbot, you know, how to add memory, how to add human in the loop like human feedbacks when you're executing the entire graph state, how to use different streaming technique, how to probably go ahead and use MCP, how to build MCP completely from scratch, right? So this part also we'll be discussing about along with this u there will be various topics like states what are graphs nodes edges how do you go ahead and use this with the help of graph API you know so all these things will be covered in part one so part one will be approximately around 2 hour 50 minutes maybe okay but I'm just making an approximate suggestion along with that once we complete this then we go to the part two in the part two cover advanced langraph concept. Now here we are going to focus on various kind of workflows and agents. Here is the topic where we will be developing applications where agents will be communicating with other agents. Right? And why they will be communicating to solve a complex workflow. Okay. Solve a complex workflow. Right? Along with this, we will try to see how we'll be handling the multistate management even in multi- aents. Then we'll also introduce you to how to directly use functional API instead of just directly going through graph APIs itself. And then I will also be showing you how you can debug and monitor them in the langraph studio. Okay, langraph studio and for this we will also be using lang. So this all fundamentals is put up in the advanced part because uh this will be like one step towards developing some amazing production grade application and finally this part two will also be somewhere around 2 hours of video and then we have in part three where we'll focus on building completely end to end projects we'll focus on LMOS pipeline we'll focus on deployment techniques and recently I have also explored all the evaluation techniques metrics specifically in LLM and how you can use along with langraph uh and some open- source tools right like MLflow how you can use AWS to track all that kind of metrics along with that how you can use graphana to probably display all those particular reports that is where we will be moving in the part three right we'll also be using hugging face spaces to do the deployment so this is just a tentative plan in order to cover lang graph crash course and these all are like long recorded videos so I definitely definitely require your entire support. Yes, now part one is ready. You can go ahead and watch this entire video and make sure that you also download the material from the description and keep on practicing and definitely do share it in various platforms like LinkedIn and all. I definitely want to see how your learning is. Definitely do tag me in LinkedIn, Twitter, wherever you can. Right? So yes, let's go ahead and enjoy this particular session. So guys, now let's go ahead and build a basic chatbot using Langraph. So this is my entire empty folder. So this will be my project workspace. Uh from this I will go ahead and open my command prompt. So let's go ahead and open my command prompt. Um as I said that this is my uh working directory uh with respect to my project workspace. I will just go ahead and open my VS code cuz I'm going to use VS code for my coding purpose. Uh once I open my VS code uh this is how my VS code looks like. Um you know whenever we go ahead and start any kind of projects or you build any applications right it is necessary that you start creating an environment. Um most of my videos I've actually shown how to create environments with the help of but in this particular video we are going to use something called as UV package manager. Okay. Yes, you can also use cond. Uh but if you don't know about UV package manager, it is a really fast, extremely fast Python package and project manager and it is completely written in Rust. Since it is written in Rust, it is very very fast. So you can probably compare over here from UV to poetry to pdm and pipsync. This has the least time. Uh that means that whenever you're trying to create an environment or do any kind of installation of the packages, that happens really really fast. Okay, some of the highlights that you can see over here. It is 10 to 100 times faster than pip. Uh it is a single tool to replace pip, pip tools, pipex, poetry, pyenv, twine, virtually envir. Uh it provides comprehensive project management and universal lock file. It installs and manages different kind of python versions also. You can do it in the same project itself. Right? And uh to start with the installation, if you are in Mac OS or Linux from the terminal, you just need to go ahead and copy this particular command and execute it. If you are on Windows, go and open your PowerShell, copy this particular command and uh paste it over there. And if you're using pi, uh just go ahead and write pip install uv. Okay. Uh once that is done, your uh you know the entire project repository will be initialized. Okay. So first of all, what I'm actually going to do is that I'll just go ahead and open my terminal. Now inside this terminal I will open my command prompt. I've already done the installation of UV package manager. So I will just go ahead and quickly initialize my uh project workspace. In order to initialize all I have to do is that I have to write uv init. Okay. As soon as I write u init will happen in the project workspace. Okay. So here you can see in the project workspace there are some files that has got created like get ignore python version main.py pipro.2ml 2 mm uh and and if you probably go ahead and see in Python version which Python version you have actually created it is nothing but 3.13 then you also have this main py so that you can start the program execution directly from here then you have this pi project toml wherein you have the project brief information uh you can change the version you can add your own description and all um here you can see that it is requiring a python of minimum 3.13 okay this dependency is right now empty because we have not installed any kind of packages is now what I'm actually going to do is that I'm going to go ahead and create my requirement.txt. Now inside my requirement.txt I will go ahead and install some of the libraries. Let's say lang graph lang chain. Then along with this I will also go ahead and use my lang. Okay. Uh all our libraries that will be specifically useful. Uh and I'll tell you as we go ahead lang and lang chain we're going to use various functionalities in order to build generative AI applications. chat bots along with that agent AI applications also uh lang is basically used for uh tracking and evaluation of your applica applications you know directly in the langraph cloud so these are the basic libraries that we're going to use now since I have already initialized this working space now the next thing is that I need to go ahead and create my virtual environment so quickly I will go ahead and write uv venv um with the help of this command you'll be able to create a virtual environment this venv is nothing but your uh virtual environment name. Okay. So once I go ahead and write uvnv here you can see that it has got created with the help of this particular version that is 3.13.2. The virtual environment is at this location.v over here. Now in order to activate the environment I'll just go ahead and copy this quickly. I will paste it over here. Okay. So once I activate this here you can clearly see that hey uh my my uh environment has got activated. Okay. So here agentic lang graph has got activated. Now this is perfect till here everything looks good. Now the next step is that we'll go ahead and do the installation of all the libraries. So in order to do it u like if you're using pip it is like pip install minus r requirement.xt. But here we are going to write uv minus r requirement.xt. Okay. So once you do this installation here you can quickly see that the installation has been completed. And now if you go ahead and open this particular file. All the libraries that has got installed will be visible over here. Okay. Now as I said uh this is my first tutorial. I'll go ahead and just write one folder name one and I'll say hey uh basic chatbot. Okay. And we'll we'll just learn some of the basic stuffs over here. Okay. Now with respect to this particular basic chatbot uh here we are going to go ahead and create our um you know applications. So we'll go ahead and create our basic chatbot itself. Um we'll just go ahead and open my one file. Let's say I'll go ahead and write basic chatbot ipynb. Okay. So as soon as I open this basic chatbot ipynb, it will tell me to select a kernel. I will go ahead and select a kernel. And since I'm using Jupyter notebook for the initial stages, um I also have to go ahead and add one more library UV add ipi kernel. Okay. So follow the steps step by step like you have to just follow this steps as we go ahead because IPI kernel will be required in order to run anything in the Jupyter notebook. Okay. Now once this is done I will start writing my code over here. Okay. U now with respect to the code let's check whether this is working or not. Okay. It should give an error because oneplus exclamation is something happening over here. Here you can see it is connecting to the kernel agentic. Okay. So yeah invalid syntax. Now if I go ahead and write 1 + 1, it is working fine. Perfect. Now here as I said um let me just quickly go ahead and write here we are going to build a basic basic chatbot. Okay. Now building a basic chatbot uh um you know this this chatbot is like a basic chatbot that basically means uh and here whenever I'm talking with respect to lang graph okay with lang graph I'll go ahead and write that here we are going to use the graph API functionality okay there is one more API which is called as functional API as we go ahead we'll also try to learn about it but what I felt is that the most efficient way of learning lang graph is specifically using this graph API Okay. Okay. Um, so let's start with this and uh let me go ahead and write some information you know how you should actually go ahead and start and all the things you know and what we are basically going to develop. Okay. So guys, now let's go ahead and build a basic chatbot with the help of Langraph. Now before we go ahead, we need to understand some of the important components of Langraph so that you will be able to understand how to build a basic chatbot. So let's go ahead and talk about the components of Langraph. There are three important components of lang graph. Number one edge, number two nodes and number three which is called as state right now what are these right? What are these components? So in order to explain you I would like to probably take a use case. Okay let's say that and I have I had this use case a long time and I solved it. You know as you all know that I also upload a lot of YouTube videos right YouTube videos. Now what I wanted was that I as soon as I upload a YouTube video I should be able to convert or create a blog out of it. Okay. So this is a kind of task that I really wanted to do. Now in considering this particular task if we consider this workflow how this workflow needs to be executed. You know let's understand this. If I want to solve this task, the first thing is that from my YouTube videos, I have to take out my transcript. Okay, transcript, right? So from this YouTube videos, I want to first of all take out the transcript. Then I will use this transcript. Transcript. And with the help of this particular transcript since I need to start writing my blog I will go ahead and create the title of the blog. Okay. And in third step I want to take both title and transcript and we will go ahead and create the content of the blog. Right? So if I want to solve this use case, you know, this will be my workflow to solve this use case. You know, first of all, I need to go ahead and take out the transcript from the YouTube video. Then I need to go ahead and based on the transcript, we need to go ahead and generate a title. And then based on the title and transcript, we need to generate a content. Right now I am alone uploading the videos and it is not possible that I also go ahead and create the blog out of it because it will take more of time. But since when like LLMs right now is the buzz word, right? We definitely have LLMs. Now with respect to LLMs, you know that these are really really good at content generation. It is very very good at content generation. Right? Now whenever we talk about content generation that basically means LLM it can take an input. Let's say if I say hey what is machine learning? It'll be able to generate what is exactly machine learning. Right? Now can we use LLM in order to solve this particular workflow with the help of langraph. Now in order to solve this problem what I will be doing is that I will follow some kind of graph structure. Okay. And yes in langraph if you want to solve this kind of workflows. There are two ways. Okay. One is directly using graph API. graph API and second one is directly by using functional API functional API but according to my experience I feel graph API is the most easiest and most best way yes if you have lot of expertise with respect to the graph API you can directly go ahead and use the functional API the difference between them we will get to know as we go ahead okay so first of all what we'll do in order to solve this complex workflow I will go ahead and create some kind of graphs. Okay. And this graph will show that how my flow of execution will happen. Okay. So let's say that I have this node I have one more node. Okay. So these are my two nodes. As I said the components of langraph are edges, nodes and state. Okay. So initially let's say we are going to go ahead and start over here. Okay. So here I will be having my start node. Okay. In this start node we give our input right. Let's say in this particular case in my use case obviously I need to give some kind of input. Now what input I will give? I will give my YouTube URL. Okay let's say this is my input YouTube URL. Then it goes to this phase from start it goes to this node. This node should be responsible in taking out the transcript from my YouTube video. So here I can go ahead and write, hey, this is my transcript. Okay, this is my transcript generator. Now, how do I go ahead and generate the transcript in Langchain? In Langchain, we have some third party libraries. No, I think there is something like YT loader or what it does is that we give our input videos of the YouTube and output we will be able to get the transcript. So here output of this particular node should be that we should be able to get a transcript. Okay. Now understand one thing over here. So what is this? This is nothing but this is my node. What is this? This is nothing but this is my edge. Right? So this is nothing but edge. Edge main fundamental is that the flow of information should go from here to here or node to node. Right? So this is also my edge. Right? Now whenever we talk about nodes, right? As soon as you create a node, we also have to create a node implementation, right? Some functionality with respect to this particular node. Like what does this node actually do? Now in this particular case this node functionality should be that it should take a YouTube URL and it should generate a transcript. Okay. And the output of this node should be this transcript. Okay. Now in my workflow I have completed this YT video to transcript by that node. Now based on this transcript I should be generating the title. So what this node will be doing this is nothing but this will be title generator. And here the input will be transcript right. The input will be transcript. And this will be my next node. And this node functionality should be that it should take this transcript and it should generate the title. Right? This is what is my functionality. Very simple functionality. Right? Now after this the output that we're going to give right should be my title. Along with the title I also want to give my transcript and we go to the next step. What is the next step over here which is nothing but content generation. So my third node that you'll be able to see over here is nothing but it is content generator content generator right. So this will again be my edge and this node will have a functionality which will take this information title and transcript and it will generate content right and finally you go to the next step which is end in the end you get the output right now see now you may be thinking how do we generate transcript to title. Now if you have a fundamental idea of LLM. So here in my title generator I will have an LLM along with one prompt and then when we give this input of transcript it should be able to generate the output. Right? Similarly for this content generator which is the node. If I give the title and transcript here again I will be having some kind of LLM plus some prompt which will be able to generate the content. Here we give the input as transcript and we get the output over here. Right? And finally all this output is combined and we get display it over here. Right? So this is an example of a workflow and this is entirely with the help of graph API. We will be able to see the graph uh we'll be able to see the execution. We'll be able to see the output. Okay. Now coming to this right I have told you already about edges and nodes right now where does state come into existence. Okay. Now see based on this particular use case state we can define something right. So here this state will have some values or some variables we can define some variables and that variables will like that variables can be accessed by any of this node in this particular graph. Okay. So let's say for this particular use case you know that I require transcript. So I will go ahead and create a transcript variable. As soon as this node is executed the output will be saved in this variable. Okay. So let me just go ahead and write it down over here. So state means what right? Whenever we define any kind of state our main aim is that whatever variables we define over here right. So let's say one of the variable I want to define is transcript because as soon as I execute this node my transcript will get generated right and this transcript will also be required in my third node. So what I can do when I create this state right this state will have one variable which will have the information about the transcript maintained. Okay. Similarly title is my third second output that I really want because here in this node I want to go ahead and save the title right. So here title information will be saved and then here you have content. So let's say that if I go ahead and create this three variables as soon as we generate those we can save in this right and the advantages of saving that values inside this state will be that inside this entire graph every node or any of these node will be able to access this variable. Okay. So that is the importance of state. Okay. And this entire graph we basically say it as state graph. So that is the reason we say it as state graph because it is able to maintain the context of the state at every node. Yes, don't get confused with external memory or memory. Right? So memory can also be used over here and that part we'll discuss in the later stages. But here we want to focus more on the state graph. State it is able to maintain the state within the specific nodes. Now I hope you got a clear idea about the components of lang graph. Now what we'll do? We will build a basic chatbot. In this basic chatbot what we'll do I will be having a start. From this start I will create one node. Let's say this particular node is nothing but chatbot. And from this we will go ahead and end it. Now this chatbot will be integrated with some kind of LLM press prompt and the work is take the input and give the output. Right? So this is the basic chatbot what we are going to build and as we go ahead you know we will go ahead and add tools external tools. We will go ahead and see that how we can integrate this external tools along with the chatbot. Then as we go ahead we'll again discuss about react agent. Okay. So react agent is something more amazing with respect to the tools. I know there are so many topics that we have discussed but let's now focus on understanding how to build this basic chatbot. So for this I will again go back to my code and now you have understood what is state graph. You have understood what exactly is nodes and what exactly is edges. Okay. Now step by step we will go ahead and do this. As usual, what we are going to do is that first of all, before starting building a chatbot using uh uh state graph or graph APIs, you know, first of all, we will go ahead and import some important libraries. Okay. So, one important library is something called as from typing import annotated. I'll talk about annotated. What exactly annotated is? It is just to add context specific metadata to a type. Okay. uh it is better that I show you an example in order to make you understand one more important library that I'm going to use is typing extension import type okay now along with this since you know that every graph starts with a start node and ends with the end node okay so for this I will go ahead and write from lang graph dot graph import state graph since we need to go ahead and also create a state graph state graph will be the entire graph right entire graph that you have seen over here. If I want to represent this entire graph, we can represent it with the help of state graph. Okay? And then comma start and then we will also have end. Okay. Start and end are just like my start node and end node. Along with this we will also go ahead and add from langraph dotgraph dot message import add messages. Okay then let's go ahead and execute this. Okay now we have imported all the libraries. Now you may be thinking kish what exactly this add messages is. These are called as reducers. Okay. Now what is the importance of reducers? Okay, I will talk about it. Let's say that if I want to create this chatbot, right? If I want to create this chatbot, you know that we also have a state, right? Now in this state, what is the kind of variable that I really need to create so that any output that is generated by the chatbot will be saved it in one variable itself. So let's say that if I go ahead and create a variable called as messages inside this messages can I make this as a list type and inside this list as soon as I keep on asking any input automatically it should keep on getting appended. So again let me repeat it what I'm trying to say over here. Let's say if I'm creating this basic chatbot as soon as I give an input this should be able to generate an output. But again in that session if I give another input it this graph will again get executed and it'll give me the output right. So we can execute this graph as many number of times. Right? So when we are creating this state graph okay state graph so every conversation can I save that inside my state right which will be available to this particular node at any point of time yes. So for that what we'll do we'll we'll create one variable. We'll make it as a list type and inside this list we should keep on adding this messages. When I say adding it should be appending this messages. It should not replace the previous message. Okay. When I say replacing the previous message let's say in the first instance I had one message. I said hi how are you? Then the chatbot replied I am good. Then my next question is hey uh tell me what is your name? Then the chatbot replies hey I do not have any name I'm just a basic chatbot so this message should not get replaced instead it should get appended you know as every conversation goes ahead so that we will be able to maintain this information and that is the reason we say it as state graph okay so in order to probably append it we can use something called as reducers okay one of the example of the reducers there are different types of reducers that we can specifically use one of The red reducer is nothing but add messages. Now this add messages what it is going to do is that its work is only to add the messages instead of replacing in any kind of variable that we define. Okay. So now let me just go ahead and execute this. And now I will go ahead and start creating my state. So here I will write class state is equal to and here we are going to use this type dictionary. That basically means the state class is going to return type of a dictionary right. So here let me just go ahead and provide you some basic dock string so that you should be able to understand it as we go ahead. So here you can see messages have the type list. The add message function in the annotation defines how the state key should be updated. In this case it appends messages to the list rather than overwriting them. I hope everybody has understood why we are inheriting type deck because this state is going to return right this class is basically going to return of this type that is nothing but dictionary type right so if you see what is type dick it is a simple type name space at runtime it is equivalent to a plain dictionary right if I'm going and writing class point 2D type dick right so x is int y is int label is str so what we can do we can provide values in the form of dictionaries right key value pairs It's like x is equal to 1, y is equal to two, label is equal to good, right? Something like this. Now in the next step, what we are going to do is that we create one variable. Let's say messages. Inside this messages, we will go ahead and use annotated. Now annotated is just like a kind of label. Okay, this annotated class that we have inherited or we are basically writing it is nothing but it is it indicates the hypothetical runtime check model. This type is an unsigned integer. every other consumer of this type can ignore this metadata and treat this type as integer. So if you see some of the examples over here, you should definitely be able to understand these are something like in Python what exactly this basically means right now inside this I will say hey you have to go ahead and add the messages inside a list type with the help of add message. So this add message is called as a reducer. Please remember this information. When we say reducer, that basically means it is not going to replace this list with respect to every conversation we have. Instead, it is going to append. Append right. So here you can see that how this state key should be updated. In this case, it appends messages to the list rather than overwriting them. So this is the basic information. But I will show you how this looks like as we go ahead because we will go ahead and just display this with respect to the state. Now I will go ahead and build my graph. So in order to build my graph I'll say graph builder. I'll use the state graph and I'll give this class right. I'll give this class. That basically means when I give this specific class over here, this state graph uh when we are creating the entire graph uh at any point of time we can provide this specific information to our different different nodes. Okay, so this basically becomes my graph builder. Here I'm just going to go ahead and give show me my graph builder. It is nothing but it is of a type state graph. Okay, so my state information has got completed. Okay. Now in the next step what we are going to do is that we are going to build our entire graph itself. Right. We going to go ahead and build our entire graph. Okay. Now for this first of all what we are basically going to do is that I will go ahead and put one more libraries. So for this I will use python.env since we are going to go ahead and use uh you know uh grock models. You can use openAI models. You can use any kind of model. So here what I'm actually going to do I'll go ahead and write uv add uh minus r requirement txt. So once I go ahead and install this the installation has been done. Now once I go over here right so here you can see that um now we can go ahead and quickly import all the libraries that we want. So I will go ahead and write import OS then I will go ahead and write from lo from env import load env right and then we're going to go ahead and initialize this load env right so the reason why we are doing this is that whatever keys we specifically write in our enenv it should be able to load it so here I'm going to go ahead and create my env file right now with respect to the env um the Next step uh that we are going to specifically do is that whatever keys that we specifically want with respect to the gro API, we'll paste it over here. So this is my env. I hope everybody knows how to create a gro API key. In order to do that, just go to console.grock. Okay. So here you go to console.grock.com, right? And here you just go ahead and create your API keys. You can go ahead and create your API key, write the API key name and start using it. Okay? So this API key we'll be using it and we can use different different um you know models LLM models in order to develop your generative AI applications. Okay. So once this is done I will quickly go ahead and again execute this since my env. So first of all I will show you one very easy way. So I will go ahead and write from langchain or sorry from langchain grock. Okay. So for this we need to install this library. It's called a lang grock. So I will go ahead and write lang chain gro. Okay. I will open my terminal requirement.txt. Now here you can see langchen gro has got installed and I will go ahead and minimize this. So from langun grock I will be importing chat gro. Okay. So this is one way you can directly initialize the gro model. The other way is more common and generic way. So where you can just give the model name and automatically it should be able to do it. So for that you will be using from langchain langchain um dot chat models import init chat model right so here if you want to directly go ahead and use your lm with the chat gro you can just go ahead and write like this and with respect to this you can just provide your uh model name okay so models it is up to you whatever models you specifically uh want to use or you want to go ahead with it, you know, you can definitely go ahead and just use that. Okay. See, at the end of the day, it's all about how you are using some specific models and which model you really want to use. Okay. So here, let's say that I want to go ahead with some other model, right? Uh for this, I will again open my let's see my playground is over here. So let's say I will be using some models like llama 3 8 billion8192. So here all you have to do is that you have to go ahead and give your model is equal to uh lama 3 lama 3 is the names correct 8b 8b 8192 right so you can basically give this particular model and if you execute it this is nothing but this becomes your llm right this becomes your llm right you can either initialize in this way or you can also directly go ahead and write something like this so here I'll be using llama llm Initiate chat model. Here we are going to give the model name. The model name will start with something like this. Grock colon you know llama 3 8 billion 9 sorry 8192. Okay. So here also you can use this and it'll also give you the same llm right. So these are both ways how you can initialize this. Uh and again if you are using OpenAI then you can use uh Langchain OpenAI and here you can just mention OpenAI colon whatever OpenAI model name you are specifically going to use. Okay now this is my LLM. So here if I go back to my graph right we have created our LLM. Our LLM is ready. Now we will go ahead and create this chatbot. The chatbot is nothing but it is just like a node right now with for every node you need to create a node definition. Right? So in order to create a node definition, I will go ahead and write definition chatbot. Let's say this is my node. And here uh here I'm going to go ahead and define my state colon state. Okay. And here what I'm actually going to do is that I'll go ahead and write return messages colon. Now see this since this why I'm returning in this particular variable because whenever I define this chatbot right it should be inheriting this state because at the end of the day I need to keep on appending inside this particular variable right and you know the state return type is type dictionary so that is the reason we are inheriting over here state colon state and when we write return message colon here we are going to invoke it with our llm. So here I'm going to go ahead and write llm.invoke and with respect to the invoke here we're going to use the state of messages. Okay, state of messages. So we are going to basically go ahead and return this uh to give you a very brief understanding. This is what is my node functionality is okay this is what is my node functionality. Here we have defined a node called as chatbot. This llm.invoke Invoke is basically giving right based on this input message. See the state of messages is what it will be my input message right as soon as we get an input message we are giving to our chatbot node and that chatbot node is going to provide the response from this from my llm and it will append inside this messages variable. This messages variable is nothing but it is the same variable that we defined in the class state. Okay. Now this is done. Now in my next step what we are basically going to do is that we are going to go ahead and quickly start building our graph. So for this we will be using our graph builder. If you remember uh what is graph builder? So graph builder is nothing but it's my state graph. So I will just remove this quickly over here and I'll just paste it over here itself. Okay. So this is my graph builder and uh with respect to the graph builder how we need to build it. Right. In my graph builder I have to have one chatbot node, one start and one end. Right? And there should be edges connected to both of them. And as I told you that we are going to use the graph API. Right? So uh for this what I'm actually going to do is that I'm quickly going to write graph builder dot add node. Okay. So this will basically be my first node. My first node name will be chatbot. You can mention anything. Let's say I will go ahead and write llm chatbot. Okay. But the second parameter that I'm going to write is about my node definition. So which is nothing but chatbot. Right? So this every node will have some node implementation. That node implementation you should be specifying it over here. Okay. Then uh coming to the next uh option is that in my graph right I definitely have only one node right this is the node that is there. But along with this I will go ahead and create start and end as my starting and end point right. So in order to create that we need to go ahead and create edges right. So first of all uh what we basically going to do is that I'll go ahead and write graph builder dot add edge. So this was my adding node. Adding nodes this is my adding edges. Add edges. Now with respect to add edges and add node here is my start. So from the start I have to go to my LLM chatbot right. So from my start I'm going to the LLM chatbot and from the LLM chatbot I should basically go where to the end right? So I will go ahead and add one more edge and this edge is going from llm chatbot llm chatbot and remember here you need to specify the node name instead of a node functionality right and this will basically go to my end node. Okay, perfect. Now see that is what it is matching right from start I have created an edge to chatbot then again it is going to the end. So this is my entire graph right. Finally what we do is that we compile the graph. So these are some of the steps when we define the graph. The compilation is necessary so that we can execute the graph. Right? Unless and until the graph is not compiled you will not be able to execute it. Right? So for this I will go ahead and use graph builder dot compile. And here we are basically going to just go ahead and execute it. Okay. Now the question rises can we go ahead and see how this graph looks like? Okay. Yes. Obviously you can see it. So for this we will be using some visualization graph. So I will just go ahead and write visualize the graph. Okay. So from visualization graph I will go ahead and write from I python dot display okay import image comma display okay so we are going to use this and uh again we're going to go ahead and use try catch block where we're going to use this display method which is responsible in displaying the graph with respect to any image object that you give and if I go ahead and write graph get graph I should be able to get the graph itself and this we will try to draw it in some mermaid png. Okay, these are some of the functionalities that were provided over there in the documentation. So I'll go ahead and write except exception. Okay, and here I can just go ahead and write pass. So here you can see this is how my chatbot looks like. So here I have start, this is my LLM chatbot and this is my end, right? So when I give my input from here, my llm my start will be sending this and I should be able to get this. Okay. Now the time is that how do we run this? You know we we really need to run this right at any point of time and if you are running it how does it basically looks like you know so for this I can directly use this graph dot invoke. Okay. And I will say hey u hi. So let's say this is the message that I'm giving. So what will happen? Hi will go from here. it'll go to the LLM chatbot. It'll give you the output and it'll end. That's it. Right? So when I say hi uh got hi. Okay. So one problem over here that you'll be able to see that uh when it is trying to retrieve the details there we are facing some kind of problems. Okay. Now what is the exact problem that we are facing? I will just try to uh resolve this uh as we go ahead you know. So let's go ahead and do this. So here one very important thing is that in the state you remember that what is the variable that we created right messages. So what I will do I will go ahead and create a dictionary called as messages. And now inside this I will give my message saying as hi before I had not given this. So it is not able to pick it up right because here if you see inside my functionality of LLM chatbot it is invoking from this particular variable right from state of messages where it is basically saved right so here now let's go ahead and execute this now it should execute it let's see invalid API key during the task okay so my env is ready okay no worries see the problem over here is that we need to restart the kernel because my API key I added it in the later stages right so that is the reason so quickly I will execute all these things sorry graph builder is not required over here now it should execute it because I just needed to reload this you know by restarting my kernel then only it'll get reloaded okay no worries now it should work so my visualization graph is there and now I'm invoking the messages of high. Now here you can see that I have got graph.invoke messages of high human message. Now you see this high that is going right. It is being treated as a human message. And now your response is with respect to the AI message. Hi, it's nice to meet you. So let's go ahead and save this as my response. Okay. Now in order to check the response, right? What was the final response here? You can see that I can go ahead and read inside my messages variable. Now this is what is really important. See inside my class state right I told you that we are going to create a variable right over here this is my messages variable annotated was there list was there and add message was there this add messages is acting as a reducer reducer work is to append inside this list see initially human gave high then AI message gave high right and this has got added inside this list understand one very very important thing And this is in the messages variable. Right? Now you may be thinking what is this annotated? Annotated basically means what? As soon as I gave hi. See over here automatically this messages got converted to human message. Right? Human message is just like one kind of annotation. We uh the the the the the graph is making sure that it is annotating and it is appending in this specific list. And the reason it is getting appended because here you can see that directly that my messages are getting appended with the help of those reducers itself add messages itself. Right? Now I hope you are able to understand it right why we have specifically defined it. Now the question rises how do I go ahead and retrieve the last message? It is nothing but response of message minus one. Okay. So here you can see that I have got this and if you just go ahead and write dot content you should be able to get hi it's nice to meet you. Is there something I can help you with? Okay, so this is the most easiest way of probably uh reading all the stuffs. Okay, now there are two more way of streaming it, right? Streaming your specific data or or running your entire graph and uh you know displaying the information, right? So that is what we will discuss now and understand one thing guys if you are able to understand this right trust me as you go ahead any kind of graph any kind of complex workflow that you have in your mind you should be able to execute it okay now what I will do I will go ahead and write for event in graph dot stream okay so this time instead of directly using graph.invoke invoke I am using something called as graph stream we will understand about this as we go ahead but I just want to give you some kind of information how things work in this so now here I will give you messages okay and colon let's say here I go ahead and give hi how are you okay so this is what is my message let's see whether everything looks fine uh yeah this is my for loop yeah now what I'm actually going to do I will just go ahead and write print event. Okay. Now let's execute this. So here you can see that inside this I have got an output which looks something like this AI messages messages AI message all these information and I'm getting this right now when I am doing graph stream with this particular input right so here I'm getting with llm chart lm chart is nothing but my uh node which you are able to see this okay now let's say that I will go ahead and write one more for loop I'll write for event or so for value in event dot values event dot values now what will happen if I just go ahead and print this okay see I will print my value now if I execute this here you can see that I'm getting this AI message right so that basically means now whenever we try to stream from this graph stream and whenever we try to see the event values only AI messages will be getting displayed Right now in order to display this what I can basically do is that I can also go ahead and write value of messages uh which will be my last message minus one and here I'll just use dot content and this will basically display the same thing like what it was displayed over here right hi I'm just language model so I don't have feelings or like human do and all this is just one specific example I've told about streaming but don't worry because this streaming we will discuss more about it there are multiple types of streaming. With respect to streamings, you can also provide different different parameters what exactly it means, you know. So, we will discuss about it as we go ahead. But here this was just an example of how you can go ahead and build a basic chatbot. Okay. Now, it's time that we start thinking crush can we go ahead and integrate some kind of external tools. Okay. So, for this let me go ahead and talk about a use case. Let's say I have a chatbot. Okay. Now this chatbot I have a question I can basically go ahead and ask a question for this chatbot saying that hey let's say this this chatbot I have and this chatbot you know what does it have it basically has a llm with some kind of prompt and it is taking an input from the start and it is basically ending it right so here start end now if I ask a question provide me the recent AI news. Do you think the chatbot with the help of this LLM will be able to provide the output? The answer is simple. No, it is not able to provide it. Why? Because LLM will not have any information related to live, right? Any live information it will not have. It may have not trained with the recent data right. So here the dependency on external tool comes right external tools comes right. So what we can basically do is that for this chatbot as soon as we give an input this chatbot should understand hey we are not able to answer it. So I definitely have to make a tool call. I definitely have to make a tool call. And when I'm making this specific tool call this tool call let's say this can be any third party API it can be uh Google search engine it can be let's say one of the search engine that we going to use is tavi tavi is nothing but it is a web search it provides a web search API okay and with respect to this tavi we will be able to get some kind of response over here okay so as soon as we make a tool call or in order to define it like this I will I will just make this graph a little bit longer now. Okay. So what what happens as soon as I get an input the next thing is that the chatbot is understanding it is a tool call. So it goes and makes a tool call and here we will define another node which will be called as tool node. Okay. And then based on this tool call I should be able to get the response in the end. Okay. So instead of chatbot not able to give you the output let's say if I give any input provide me the recent AI news this request will go to the chatbot chatbot will understand hey we do not we do not have that information so definitely I have to make a tool call so here what it will do it will make a tool call and then from here it'll go to end okay it'll go to end right and here in this tool node I may have multiple tools I may have tools like tabuli. I may also go ahead and define some custom tools. Let's say add, subtract or some custom implementation also you can go ahead and write. Right now the question arises how does this chatbot knows about the tool node? See there is something called as LLM. Okay, inside this chatbot we use LLM, right? LLM is actually the brain behind taking this decision. Why this LLM can be binded with this tools. When LLM is binding with this tools, what does this basically mean here? It means that let's say I go ahead and create one custom function. This custom function is called as addition. Let's say this is my addition. This can be added as a tool to the LLM. It can be binded with LLM itself. Then the LLM here whenever you define any custom tool you also need to provide the dock string. You need to provide the dock string. Now with the help of this dock string the LLM will know what are the inputs and what are the arguments that is required over here. So if this inputs and arguments matches with the input that we are giving in this chatbot then automatically this is going to make that particular tool call right so this same thing I will try to show you it in the practical way we will go ahead and create some some tools we'll also go ahead and create some of the custom tools and once we do that what we are basically going to do is that we also going to go ahead and create tool node and There is one more additional condition which is called as tool condition. So we will discuss about all these things with respect to this particular implementation. But our main point over here is that the chatbots can also be integrated with a separate tool node. And here it can also make a tool call based on a specific input that we get. Okay. How that can be implemented? by binding tools with the LLM and also defining your custom functions if it is required and this LLM will be able to understand whether it has any tool or not through this dock strings. Okay. So now let's go ahead and implement those functionality. So guys, now let's go ahead and start building a chatbot with tools with the help of langraph. Now first of all, I'll just show you like what we are trying to build over here. Okay. So here is one graph uh you know uh which we will try to create. Now just observe this graph. Okay. This graph is quite amazing because here uh we have a separate set of tools. Okay. Here we have a tool calling LLM. Okay. So from here uh we are definitely going to give our input. Now from this input as you know these are my edges. This tool calling LLM is my first node and this node has LLM. LLM with binding tools. Okay, when I say binding tools, what does this basically mean? So this means that I have LLMs and tools integrated with themselves. We will use couple of tools. One of the most famous tool that we will try to use let's say we will use Tavly API. or tably search. This is just like an internet search. Along with this, we'll also create some of our custom functions. Okay, we will uh create some of the custom functions or custom tools. Okay, tools. And remember here when I can also combine multiple tools in one tool node. Okay, so here what we doing is that we're going to combine this in one tool node. Okay. So this is nothing but this is a tool node and here you can observe one more very amazing thing right. So from this particular tool from this particular node here we have two paths either we can go there here or either we can go over here. So let's say if my question is hey what is the recent AI news? So the input will go over here. Then this tool calling LLM will decide whether it can give the answer or whether it is dependent on some tools. Since we have binded this tools and remember how LLM will be able to understand from the dock string, right? So in every tool there is some kind of dock string. Dock string is nothing but some brief information about what that tool actually does. Okay, I will also define one custom and show it to you. Then this tool calling LLM you know since it has those tool information it will take a decision whether it has to make a tool call or whether it can just answer it and go to end. Let's say if it makes a tool call the tool will then provide some kind of output message and it will end. If it is not a tool call it is just going to go and give you the output and go to the end state. Okay. So this is uh fundamentally a simple problem things that we are going to solve right now. Okay. And we'll solve it step by step. Okay. how how do we go ahead and solve it? Uh that I will discuss as we go ahead. Okay. So now let me go back to my code. So first of all in my requirement txt I will go ahead and import one library which is called as langchain_tavly. Okay. Now langchen tavly is nothing but uh if you see in my envi api we need a tavly api. Okay. And in my requirement.txt we need to first of all install this. So quickly let's go ahead and open my terminal and here I will go ahead and write uv add minus r requirement txt. Okay so once we do the installation the installation will be completed and uh we are good. We have this lang tabi. Uh the next step will be that I will just go ahead and open this website called as tabi. Okay. So here you can just go ahead and search for tabi.com. It empowers your AI application with real-time accurate search results tailored for LLM and RAG. It's just like an internet search. Okay. So, I will just go ahead and log in. Once I log in here, you can see that it'll give you one key. I will copy this key and it is free for free. Uh you can probably hit many number of requests with the help of this. So, I think you don't have to be dependent on my API key. Right. So, I will go ahead and write tab API key and I'll paste it over here. Right now the next thing is that uh since I'm working over here with with the help of this you know I will just go ahead and restart my kernel. Okay you have to restart your kernel otherwise things will not work you know the reason is very simple because we need to import this again. So I will first of all go ahead and execute this. This will basically be my lm. Okay this or this can be your lm no worries. Okay now I'll go back over here. Now let me go ahead and import some of the libraries. Right. So for tabi first of all I will go ahead and import this tool. So I'll write from langchain tavly. Okay. Uh I'm going to go ahead and import tavly search. Okay. I will go ahead and create this tool wherein I initialize the tavly search and here my max results is equal to two. Okay. And then I will define my tools. Let's say this will be my list of tools. Okay. I can still define many number of tools I like. Okay. But I'll use this tool. Let's say I will go ahead and just invoke with one message. Let's say I will write what is no or what is lang graph. Okay. So this will basically be my question. Now once I execute this here you should be able to see some kind of response. So here you can see what is lang graph results. You are able to see all these values title from different different source and URL you are able to see this right langraph is a Python library and all this information is specifically coming up. So once uh we have created this sav tavly search tool now our next step will be that uh we will just go ahead and try to create uh our custom method okay custom function so that gives you an idea like how you can also integrate a custom function and how lm is able to understand because I spoke about something called as dock string right so how do we write this dock string everything we'll discuss right so let's let's take a basic function so here uh I will define one custom function And this custom function here we're going to just go ahead and write definition multiply and let's say here I will go ahead and write a int b col int and let's say this is going to return type of int right so u now the question rises how do we go ahead and write our document string so this basically gives you a document string example okay here in the summary let's say I will go ahead and write multiply a and b okay and And then here uh let's say a will be my first int b will be my second int and it returns an output int. Right? So it is something like this. I've just written some information. Now this is what is called as dock string. Okay. Now this dock string will be very important because once we bind any functions right with our LLM or this functions can also be converted as a tools and bind it right and then LLM will be able to understand what this tool will be able to do it now what I will do I will go ahead and create my variable tools here I'm going to use first tool multiply and let me just go ahead and execute it right now as I told you that I need to bind this entire tools this list of tools with my LLM So I will go ahead and write llm dot bind tools and here we're going to basically go ahead and write tools and this will be nothing but llm with tools. So once I go ahead and write so here now if you go ahead and see this is nothing but llm with tool right. So this is nothing but it is a run runnable binding chad grock. It has all the information over here and uh what all functions it is basically connected to like it is connected to tavly search it is connected to multiply you can find out all those specific information over here itself right and this is how uh things work in this. Now once we have defined this llm with tool this tool we are going to use inside our chatbot node. Okay. So now let's go ahead and create the entire state graph right remember the structure of the state graph how it will be I have start I have tool calling lm this is connected to tools and this is end okay now our question is that how do we basically create this tool nodes also and for this also we have some predefined uh packages uh available in langraph okay so I will quickly go ahead and write from langraph from langraph graph dot graph as usual. I'm going to go ahead and import state graph. See again I'm importing all these things so that you get to know like I know in the top already we have imported it but you should know what all things are there. So from langraph dotp pre-built I'm going to go ahead and import tool node right see we have anyhow binded this llm with all the specific tools right so binding will play a very important role see there are two important things one is binding when we are binding llm with tools this actually helps the llm to understand which all tools it has which all tools it has right so whenever an input comes it's just like just imagine LLM has some kind of weapons to solve your input right if I ask hey provide me the recent AI news obviously LLM will not be able to do it it will do an internet search and it will try to provide you the response right when we do this binding it is just trying to give you an information that LLM has all the specific tools but further when an LLM makes a tool call it has to make a call to this tool that is what we really need to understand how that tool call will be happening okay so I'll go back over here we have imported something called as tool node now all the tools that we have created these all tools it has some kind of functionalities right these needs to get converted into a tool node Okay, because each tool node will be having some kind of implementation. Along with this, we will also go ahead and import one more library from langraph.prebbuilt. import tools condition. Okay. Now, first of all, we will go ahead and start with the node definition. Okay. Here we are going to create a uh definition. Okay. And before creating a node definition also first let's start creating the graph. Okay. So you know first of all we going to use a builder. This will be of type state graph. State graph. And inside the state graph we will be using something called as state. Okay. This will be our class specifically state class. Then in the next step is that we going to go ahead and create our builder. Add node. So two one node uh two nodes we definitely require if you see in this graph one is a tool calling llm and one is the tools right. So first we will go ahead and create this node. Inside this node we will give the name as tool calling llm. Okay, and then I have something called as tool or I have to define the functionality of this node. Right? So this in the later stages will define still I'm not defined because I will be defining it over here. The other edge that we really need to create or other node that we need to create is nothing but add node is nothing but tools and remember this tools will be nothing but it will be of node type of tool and here we are going to give all our tools itself. So this node is nothing but it is this specific nodes and inside this node if I want to go ahead and write a definition it will be of tool nodes. If you go ahead and see the definition of this tool nodes, it is how all the list of tools that we specify, it will be implemented as a tool node itself. Okay. So this is my node name and this is the definition. This is my node name and this is the definition. Now let's go ahead and create the definition. So for creating the definition, I'll go ahead and write tool_calling lm. And here we are going to define state colon state. And here we are going to go ahead and write return messages colon. Again what we are going to go ahead and write here we are not directly going to call lm but instead we are going to call lmit tool right. So here llm tool dot invoke and where do we get the input from? from state of messages. Right? So here we are going to go ahead and define state of messages. Perfect. So here you can see very clear. Now in the next step we are going to go ahead and add the edges. Now adding the edges is really important. If you understand this any kind of complex use cases you'll be able to understand it. Okay. So the first edge is from start to tool calling LLM. Okay. So first of all let's create that. In order to create it uh we will just go ahead and write something like this. See builder.addage start to tool calling llm. Now from tool calling llm there are two nodes that are going on right sorry two edges. One edge is going to the end and one edge is going to the tools. Right. Now this kind of edges are called as conditional edges. Okay. So in order to add a conditional edges it will be like builder dot add conditional edges. And inside this we are going to call our tool calling LLM. From two calling LLM this will happen right from this specific node it is going to happen. So tool calling LLM and in the next this is really important. Okay in the next we are going to import something called as tools condition. Now the question rises Kish what is this tool condition? Tool condition applies two different kind of conditions. One is let me go ahead and write it over here. If the latest message right in the input message when we giving from the assistant from the if the latest message from the assistant is a tool call then tool condition routes to tool node. So tool node is basically created over here. Right? If you create with this other name this will not happen then. Okay. So that is the reason we have created this tools node. Okay. If the assistant is saying it is not a tool call then it will go to the end. that basically means this will serve and it'll go to the end. So this tool condition basically applies two different condition. If the latest message from assistant is a tool call, tool condition routes to tool. If the latest u message from the assistant is not a tool call, tool condition routes to end. And that is where you are actually doing this with help of tool condition. Okay, very simple. Here if this tool calling LLM is making a tool call, it will go to the tool node otherwise it will go to the end node. That is what tool condition does. Okay. And uh that is a kind of see whenever there are two edges coming from a node it has to go inside this additional conditional edges. Add conditional edges. Okay. Now finally I will go ahead and add the final edge builder dot add edge. Then you know where this add edge should go right the final edge will be nothing but it'll be from tools to end right the other part sorry this is a keyword so other part is that by default if it is not a tool called it is anyhow going to go to the end okay so this is actually managing the other condition now finally we will go ahead and compile the graph compile compile the graph. After compiling it, uh let's go ahead and write it out. Graph is equal to builder dot compile, right? And then we going to go ahead and view the graph. Okay, so for give viewing the graph, it will be nothing but use that same function called as display. And here we go. Uh state is not defined. Okay, state is not defined. Let me go ahead and again I think I restarted the kernel, right? So that is the reason we got that issue. So I will just go ahead and execute this. Okay, this two thing I'll execute it. Perfect. Now this should definitely work. So here you can see that I'm getting one error node already present. The thing is that I did not define the node definition over here. Okay. So let's go ahead and define this and execute it. Okay. Uh image is not defined uh because I need to import the image library. It's okay. No worries. I will do that. Okay. Now here you can see I've got the same image start tool calling LLM tools and end. Okay, now it's time we see that how we can probably call this okay quickly. So I'll go ahead and write messages is equal to or I'll just go ahead and use the same graph graph invoke. So we know there is an invoke method and here we will go ahead and give our messages parameter and I will give my message. Hey uh I'll say hey what is uh what is the recent AI news right now with respect to this you know if I'm executing this right it definitely needs to make a tool call to my um you know to the uh to the third party API with respect to tavi now here you can see this is lovely see clearly you are able to see what is the recent AI news so here is the human message that has got appended in the AI message. It did not respond anything. The content is empty. But it is saying that the LLM has made a tool call, right? Tool call. The ID is this. The function name is this. And this is the query, right? With this particular topic news, right? And here you are able to see this. And finally, the tool message that you are getting recent AI news, follow-up questions, all this information that you're able to see. Okay? Now we need to see what information is able to see right. So now I will just quickly save this in some kind of response. Okay, response. I'll execute this. Let's go ahead and write this response. So this response is basically coming like this. I will go ahead and see my messages. Messages. I will take the last message. It should definitely be a tool call. So tool message and if I just go ahead and write dot content I should be able to do this recent AI news was the query follow-up question is null and this is all the information Nvidia self-driving software platform all this news information is specifically coming if you want to display it in a much more better way I can also go ahead and write something like this for m let's say whatever response I'm getting response of messages okay response of messages from on this I'm just going to go ahead and write m dot pretty print okay pretty print I know so here you can see what is the recent AI news it made a tool call of tably search and here is my query with respect to all the response that I'm able to get right now the question rises kish uh did we go ahead and test some other things so let's test one more thing one more tool we added right what is 2 * 3 or the multiply function. What is 2 * 3, right? And we will try to display the same response over here. This time it will make another tool call. Okay? And that tool call will be nothing but it will be a multiply tool call. See multiply tool call. Now how it is able to do it? Because LLM has that binding information already, right? And it is able to make this specific tool call in a much more easy way. But still there is one very important thing. See tool message is coming up something but the operation is not happening. Right? Why why it is not happening? See over here you can see that what is 2 multiplied by 3 or I'll just go ahead and ask what is 5 * 2. If I'm executing this okay so here you will be able to see that 5 multiplied by two it is not probably producing the right kind of output. So one interesting thing you could see guys over here when I'm multiplying here the output is null right then I got to see that there was some mistakes that we did we did not go ahead and write the definition so return a multiplied by b okay so now I'll execute this this will be basically be my tools tool lm binding tools so this will be my llm tool now uh let's see I think now it should get executed so five multiplied by two okay still I have to go ahead and recompile my graph. Okay, so I'll go ahead and recompile my graph. Now if I just go ahead and execute it. Let's see it'll come. So now you can see uh tool call has made 5,2 argument it is able to find out and tool message is nothing but name multiply and answer is 10. So this kind of issues smaller issues may come but you need to go ahead and fix it. Okay. But now one more important thing is that what if I just go ahead and write something like this. Okay, see this. Okay, what is 5 * 2 and then add 10. Okay, or let's say I'll go ahead and say then multiply 10. Okay, if I go ahead and execute this here, you'll be able to see some kind of messages. Let's see. So here you can see first what is 2 multiply by two and then multiply by 10. So multiply 52 10 2. Okay, it is able to capture the argument. It is able to find out this multiply and here also we are able to get it right. So what is 5 m* 2 and then multiply by 10? It is able to find it out. Okay. Now see I will again change this. This is also working. give me the the recent AI news and then multiply multiply 5 by 10. Now if I execute this with this kind of query now just think over it you know what is going to happen. So here one very important thing happened right? Give me the recent AI news. In this particular sentence, there are two two important sentence itself. One is the give me the recent AI news and then multiply 5 by 10. With respect to the give me the recent AI news and then multiply here you can see tavly search is done but after that it gave the output and it came out. But what about this particular query right now what has actually happened? See if this is my LLM. Okay, this is my LLM or this is my chatbot. Let's say here I asked question, what is the recent AI news? What is the recent AI news? And I asked multiply 5 by 2. Let's say I asked this two question. So in a sentence there are two questions, right? LLM as soon as it got the input, it made a tool call. The tool call was in a tool node. Why it make a tool call? Because here you can see that it is asking for the recent AI news and it knows that in the tool call it has Tavly API. Okay, Tavly and then from here it went to the end node right this was start this was end. But what about this particular question multiplied 5 by two right and this is how was my entire graph don't you think if we made some kind of changes then this answer will also be able to come and what was the changes here instead of once the tool node gives you the output can't we give that output back to an LLM instead of sending this output to the end state. Now once we make this response back to the LLM then the LLM will be the main decision maker and this decision maker will help them to probably take up the next query multiply 5x2 and then it can again make a tool call because here I have my multiply function also and then once it gets the response it'll give it back to the LLM and it'll to combine both the output and give it till the end of the output. Give it at the end of the output. So this way of interaction of LLM with tools, right? It specifically uses a a very important kind of um you know there is a there is a very good communication that happens between LLM and tools and we use a kind of agent which is called as react agent. Okay. And this react agents plays a very important role altogether. Right? Now, first of all, what exactly is this react agent? You need to understand. Okay? Let me just go ahead and explain this in a better simpler example. Here I definitely have an LLM. Okay. Let's say this is my LLM. I ask a question. Okay. And you know that this LLM is nothing but it is the brain right. So here it is the brain right when I say brain this will be responsible in making the decision which tools to call and in the LLM I have some kind of binding tools. So here we go ahead and start. Here we go ahead and end right and here is my tools node. This is my another node. Okay. So let's say here I give my natural input. The natural input is that provide me the recent AI news. And along with this sentence I say hey multiply 5 by 5. Now LLM when it takes this specific input it breaks this into two sentences. So first it will try to serve this AI news. As I said this is the brain right? So what it does it knows it has to make a call to the tool node. Now with respect to the tool node it will get an output and instead of giving to the end what it will do it will come give the response to the LLM. Now the LLM will still have the second sentence context. Then what it will do? It will again make a tool call node. Why? Because this is five multiply by five. Right? So multiply is again there. It'll again go ahead and hit this particular tool node and again get the response. Then it will go ahead and see hey is there anything left in this particular sentence? Nothing is there. So what it is going to do? It is going to summarize and give you the output at the end. So this way of communication right this agent architecture is basically called as react agent architecture. In react there are three main key terms. One is act, second one is observe and third one is something called as reason. Act basically means whenever a input comes the LLM will be able to make a tool call. Right? Then when the output of the tool comes the LLM will observe okay the LLM will observe do I again need to make the tool call or should I directly go to the end let's say if this is a question again coming after that then again it makes a tool call okay and then again it is going to get the output reason basically means after it gets the output what the LLM should do that LLM is making the decision right and this is where your agent architecture comes into existence. That is where your agent behavior comes into existence. And this was the rise because of this now agentic AI has become very much popular. Okay, that is the reason why it has become really really popular. So in order to just implement this see I will I will just give you an example. So here if I want to go ahead and just use this react react agent architecture. Okay. how we are basically going to do this. Okay, I will just go ahead and use the same thing. See, I will use the same state graph this agent. Now you should tell me where the changes should happen. Okay, I will copy this over here from the tools. Instead of going back to the end, it should go back to tool calling LLM. Yes or no? Just think instead of going from tools to the end, it is now going to the tool calling LLM. Now how my diagram will look like? This is how it looks like from start tool calling LLM goes to the tool and again goes back to the tool calling LLM. And this can keep on repeating unless and until the answer is completely satisfied and the LLM is basically making the decision. Now if I go ahead and ask this question. Now see the magic. Okay, see the magic how good the output will come. Okay. So here if I make if I go ahead and probably just show you the output. Give me the recent AI news and then multiply 5 by 10. Now see LLM how it is going to behave. So give me the recent AI news multiply by this query tably search is happening perfect here's the recent AI news after this what has happened the response has gone back to the lm and then now multiply 5 by 10 which is nothing but 5 * 10 which is 50 and this is how your entire react agent works right and I hope you're able to understand this with this beautiful example that I have considered over here right and this is with respect to the react agent. So I hope you are able to understand this. Now you can keep on adding any number of tools. The LLM will be the deciding factor which tool to specifically call. So guys now we are going to go ahead and implement about adding memory in the agentic graph. Uh so whenever you create a graph you know uh langraph has a feature wherein you can go ahead and add memory and this memory actually solves a major problem you know that is nothing but persistent checkpointing. Now why do we specifically use this memory? Okay, so let me just give you some examples. So already if you know that uh we were able to invoke it from the previous uh graph that we have actually created. Now let's say that I will go ahead and ask a question. Hello uh my name is Kush. Okay. So let's say this is what I'm communicating with my chatbot. So my chatbot should be able to give me a good answer, right? It is going through this entire graph. uh over here the tool call is not required so directly it is going to the end after giving the answer right so let's say that here I've just asked hello my name is kish and it is able to probably provide a nice response saying that nice to meet you how are you today now what I will do I will again go ahead and ask a question what is my name okay what is my name what is my name so now what it is basically going to happen is that you see like what kind of response uh we will be able to get it over here. Now, what is my name? It is making this tool call. I apologize for the mistake earlier since the tool ID yielded. I will assume you're asking about your name again. Unfortunately, I don't have any information about your name and it's not provided in the conversation. Can you provide more context or clarity what you mean by name and all? So, see, I just now told my name and it also told me that hey, nice to meet you. How are you today? And now when I'm asking the same question, what is my name? It does not know. So it is not persisting that entire information uh with respect to the previous conversation or previous interaction that we had. Now lang graph has a very special property in order to overcome this advantage which is called as memory. Now for memory what we will do is that we will let's say that I'm going to use the same graph. Okay. So I will copy this and uh let's say I go ahead and paste it over here. Okay. Now once I paste it over here, langraph has a feature wherein you can create a memory saver checkpoint. Okay. Now how do I go ahead and create it? So here what I will do, I will just go ahead and write from langchain uh sorry lang graph dot checkpointer dot memory. Okay. And here uh we are langraph checkpointer memory. We going to go ahead and import. So let's see whether the spelling is correct. checkpointter domemory. So let me just go ahead and use this. And here you can see from langraph do checkpointer checkpoint dotmemory import memory saver and we go ahead and initialize this memory saver. Now what this exactly memory saver is it is nothing but it is an inmemory checkpoint saver. This checkpoint save stores checkpoints in memory using a default dictionary. Okay. So here if you go ahead and see that what it is going to do is that with respect to every node that it executes you know it is just going to go ahead and save all the information so that you can recall this particular memory again and again whenever it is required based on the previous interaction. Okay. Now where do we add the specific memory? This is really important. So here we have created a memory object. Where do we add it? While we are compiling there is a parameter which is called as checkpoint. We have to add this memory over here. Right? So once I go ahead and execute this now, now you can see that I have this exact uh right thing. Now what I'll do, I will just go ahead and u give some input. Okay. Now see if I want to use this specific memory, right, for a previous interaction or probably I want the context of the previous interaction. First of all, we need to go ahead and create a thread ID. This thread ID will be important because it will be related to one specific session. So we will go ahead and create a variable. Let's say I will just go ahead and create a web list. So this is memory obviously. Okay. And now I will just go ahead and create one config. Okay. Inside this config we will be using a key which will be called as configurable. And inside this configurable we are going to create one thread. And this thread any ID or any number that I'm giving it should be unique. Let's say I'm going to probably a user has joined a session. So I will go ahead and make a thread for that particular user. Okay. And this here is the configuration that we need to give right configurable key and there should be a thread with this particular key value pair. And this should be unique. So once I have provided my unique thread id now what I'm actually going to do is that I'm going to use this graph and I'm going to call the invoke method. Okay. Now once I call the invoke method here uh I'm going to basically give it in the form of keys right dictionary pairs. So here I'm going to basically go ahead and write messages. And now if I give the message saying that hi my name is crush. Now see what will be the magic that will happen. Okay. So here huh apart from this right the messages that we are giving we also have to make sure that for which thread ID I am providing the configuration. So here there will be one more additional parameter which is called as config and we will provide this particular config. Right. So once we get the response response I will just go ahead and print this response. So this will be graph uh let's go ahead and print it. Okay response. Perfect. Now I should be able to get my uh output. So here you can see that output is nothing but this all information is there. Hi my name is Kish and it says nice to meet you. All this information is there. Right now what I will do I will quickly go ahead and write response or let me do one thing because I think it got appended two times. Okay, I'll execute this once again and let's just execute it for one time. Okay, so with this particular thread ID, we will just execute it for one time. Um and now here you can see that I'm getting one human message, one AI message. Nice to meet you. Now if I just go ahead and see the last messages, so it'll be messages of minus one. So here you can see that if I go ahead and see this particular content, I should be able to see the output. Okay. Hi, nice to meet you crush. Is there something I can help you with? Okay. Now let's go ahead and again use the same config and let me now ask hey what is my name? Okay. Now let's see whether it'll be able to remember or not because we have already used memory saver and it is uh putting everything in that uh memory saver itself. Right? So the previous interaction context it'll be able to remember it. Hey, what is my name? Okay, so I'm just going to go ahead and do this and we are going to print this particular output. Okay, so we are going to print this output and remember we giving the same config. Okay, see when you create a end toend application this dynamic uh uh you know ID thread id will be maintained in the session itself. So that way we'll be able to maintain this entirely in the memory saver. Right? So here now it is able to understand that hey your name is crash. Okay. Uh, so this is really nice, right? Now it is able to remember. Do you know what is my name? Uh, hey, do you remember me? I'll just go ahead and write like this. Remember me? Right. This is the beginning of so I don't have previous memory of you. I have my large language model and all. Okay. Do you remember my name? Let's let's go ahead and ask this question. Do you remember my name? So it will be able to remember me. My name at least. Yes, your name is Kush, right? So it is able to answer that right. So guys now we are going to discuss about streaming and lang graph. See most of the time whenever we want to probably invoke or chat with our chatbot we were basically using this graph.invoke method right and somewhere we also use stream right now let's go ahead and try to see like what are the different streaming techniques to probably get the response uh from the chatbot itself when we executing a graph. So first of all what I'm actually going to do is that I will go ahead and uh implement some of the things like let's say I will go ahead and initialize my memory saver. Okay. Now inside this memory saver we are basically just creating a memory object. I will go ahead and create one node definition and this node is nothing but the name is superbot and here we are going to use llm with tool.invoke. Okay or I can just go ahead and use llm right? I'll just create a simple graph to probably show you what are the different types of streaming that is available over here. Right now let's go ahead and execute this now. Here is my entire chatbot node that is available over here. Right now I will just go ahead and create my entire graph. So let's say this is a very simple graph wherein I am trying to create a node called a superbot. The functionality is nothing but superbot here from start to superbot superbot to end and then we are compiling it with a checkpointer memory right and this is how my graph looks like very simple graph I think uh we are learning a lot right out over here uh from that much time like in this entire session we have understood how to create different different types of graph now what I will do I will go ahead and create a thread let's say the thread is one I'll say hey my name is Krish and I like cricket and I will give this particular config and I'm just going to go ahead and invoke it. Okay. Now when we are invoking it, you can see there are some information that you are seeing, right? One is human message, one is the AI message. AI message is basically the response. Now with respect to this, we are going to learn about three some streaming techniques. Okay. And this will be very very handful when you try to develop some kind of chatbot. Okay. So inside the streaming you have dot stream method and a stream method. The methods are sync and a sync method for string being back results. And inside the stream and all stream method, you have this two parameters. One is value. Okay. And one is nothing but updates. Now the question rises what exactly is the difference between values and updates? So in order to make you understand, let me go back over here. Okay, let's say I have a and this is related to streaming, right? to in order to make you understand what is the differences between value and updates that is what we are going to discuss okay so let's say this is my streaming right streaming topic so first of all let's say I have this graph inside this graph I have various nodes let's say I have node one I have node one I have node two that gets executed and then finally I have node three and the flow of execution is in this direction right and we are discussing about stream and earth stream methods there is a stream method then there is an earth stream method in order to understand the difference between stream and earth stream this is like specifically used for a sync okay now if you know Python I think you should get an idea about what is sync and a sync basically means right but the main important point that I'm really interested in is understanding about modes So inside this method you have two modes. One is update mode and one is value mode. Okay. Where is value mode. Okay. Now what is the difference between update mode and value mode? And we will play with this parameter. Okay. This is an additional parameter we give. Let's say over here in node one as soon as the node one gets executed here my messages variable will be equal to let's say hi. Let's say my LLM gives a high message when node one is executed. When node two is executed, the messages will probably have another information like my name is okay. So this will be my another information and when node 3 executes my current message that is being getting updated okay is nothing but crush. So this is a very simple thing. When node one gets executed, my uh current output response is high. Then node two gets executed, my current response is my name is. And when node 3 is getting executed, it is nothing but kish. Okay. Now if I use mode is equal to update, only the message that is currently getting updated only that message will get displayed as an output. Okay. Let's say if node one is getting executed if I just go ahead and print or do the streaming with respect to mode is equal to update only this message will get updated right let's say after executing all these three nodes this is the message that is getting executed again I go ahead and give my another input then this message will get generated then this message will get generated whereas in the case of values you know in the first case I will get a message as hi okay but when again I give the message this message is equal to high will also get appended ended and it will come as my name is in the form of list right so this is basically getting appended over here right when I try to stream with the help of mode is equal to value similarly when I go to my again I give an input and execute all the specific nodes then here you'll be able to see that I'll get another message which will say hi my name is Kish right so this is how it gets executed right here in a specific execution what is one of the message that gets appended or that gets uh displayed that only I will be able to see it okay so that is a basic difference between mode is equal to update and value but if you still have confusion we'll try to understand this uh with an example over here okay so now what I'm actually going to uh specifically do is that uh now uh you can see over here that I have this okay my name is this and all okay now what I will do I will use the stream or stream method whichever method you specifically want we can use this okay so let's say that I go ahead and create a thread and this particular thread has for chunk and graph builder dotstream and I'm giving this message my name is Krishna I like cricket I've given this particular config that is nothing but with thread and this time I've used stream mode is equal to updates okay so there are two stream mode one is updates and one is values now with respect to updates if I just go ahead and print it okay now see what will be the output that I'll get okay so it shows that what is the current execution AI message that is what I'm actually getting. I did not get the human message. See, focus in this. Whichever was the last message which came from the AI only that is getting displayed. But if I just go ahead and execute the same thing, instead of writing mode is equal to updates, I will go ahead and write mode is equal to values. If I execute it here, you can see human message. Here also you can see two time human message has got appended. And if you for go forward, your AI message will also get appended over here. See AI message, right? So all the conversation is basically getting updated right when you whether you give an input whatever output you get here specifically output you're getting okay here specifically output you're actually getting right again again let me repeat this over here you'll be able to see that whatever output you get after any node and if you try to stream it only that is basically getting displayed this is the AI message over here whereas in the case of mode is equal to value everything is getting displayed your human message your AI message everything is getting displayed so that is the basic difference between this modes and values. Okay. So now I hope you get this clear understanding. Okay. And let's say that I go ahead and add one more message. Okay. I'll say hey um see I executed this two times, right? This is the first time. This is my human message and here also I got the AI message and everything is basically getting updated. Let's say I go ahead and add one more method or or or I just go ahead and create one new key. Okay. Okay. So let's let's create this. Okay. And uh you'll be able to understand this very clearly. So here I will just go ahead and use thread is equal to 4. Okay. I'll say hi. Hi my name is Krish. I like cricket. Okay. Let's start from fresh. So here now mode is equal to update is update. Now I will be getting the AI message over here. Obviously since I've used this. So I've got the AI message. Now again I will go ahead and use another message over here and I'll say I also like I also like football. Okay. Now see what will happen if I make this updates to values. Okay. Now see if I go ahead and print the churn I'm getting the human message. My name is Kish. I like cricket. So this is saved in the memory. My next prompt will be something related to I also like football. You can see this. Okay. So let this get printed. It is still executing. So right now I got this human message. In the next sentence the previous conversation has also got attached. Right? Previous conversation has also got attached. And then probably after some time when this gets executed you'll be able to see that one more message will get appended and that is related to human message. See my name is Kish. I like cricket. The previous one along with that uh hi Kish nice to meet you. So you also like cricket which team do you support? He's asked the question and if you go forward here you can see AI message a sport fan with diverse effect. Now see here somewhere human message I also like football has got added and here you got the response. So values what it is doing is that it is keep on adding all the conversation inside this and you're able to stream through that entire information. Sometime this becomes uh very good in use cases where you are focused on understanding about things and all right and uh if you want some more detailed information and all now there is one more uh method which is called as a stream methods okay and for this you just need to probably go ahead and use like this see I'm using another thread id let's say thread id will be five here we are using graph builduerstream events and uh here you can use the config version each and information. If you just print this particular event, no more detailed information on different different things, different different events. So there are multiple events that are present over there. Right? So if you want much more detailed information just to do the debugging and all with respect to every sentences, you can specifically use this streaming technique. Right? So now guys, we are going to discuss about a new topic in langraph which is called as human in the loop. Now human enabler loop can also be called as human feedback. In order to explain you, let's make sure to take an example. Okay. So let's say that uh I have a specific example. Let's say I will just go ahead and draw one of the you know the same thing that what we are specifically doing right let's let's consider that here I have this start node then I have one more node. Let's say this is my lm zip tool. This is my tool node. And finally this is my end node. Okay. Now here we know that let's say that here we have this start. Okay. So this is start start. Let's say this is my chatbot. This chatbot has been binded with multiple tools. This is my tool node. Uh when I am creating various tools and we we've created one one tools such as Tavi, right? we use tavi let's say along with tavi we will go ahead and create one more custom tool and this is tool is related to human assistance human assistance that basically means whenever I try to give an input let's say this is my input when it goes to this chatbot which is binded with lms uh sorry with multiple tools where we have llm binded with multiple tools so here we have llm with tools so based on this input if this makes a specific tool call and in this tool call instead of making a call to the table if it makes a call to the human assistance. Okay. Now in response the human assistance should provide some kind of feedback some kind of feedback and then the chatbot should continue the execution. Okay. So this is what we will try to execute it you know and this feedback can be very much necessary you know uh we can we will take a very good example let's say if there is some complex workflow and in that particular workflow unless and until a human do not approve that workflow should not be completed right um let's say there are two nodes one node is executing here we can interrupt we can interrupt with a human feedback if the human gives a good feed feedback saying that yes or continue it should go ahead with the execution. Okay. So let's take this example and show it to you so that you get a clear understanding. So first of all I've created a new file. Okay. So here you can see that this is very simple. We are just loading the model uh which we have already discussed. Here you can see we are using tavly search tool type deck memory saver state graph start add messages is all about your reducers tool condition tool node. This is the two new libraries that we are going to specifically use. Okay, one is command and one is interrupt. Interrupt basically means we are interrupting a workflow. It is forcefully interrupting so that a human can provide a feedback. Okay. So here is my state. Here I have used annotated with list and add messages. We have initialized the state graph. Here we also imported one tool library. This tool library is useful because here we will define a function and that function gets converted to a tool and this tool can be binded with the LLM. So here we are defining a tool uh which is called as human assistance. It takes a string and returns a string. Here you can see dock string is given request assistance from a human. Human response interrupt query with this. So here we are interrupting with query. Query is equal to query. So whatever query we pass over here that query it will get interrupted and then we are returning human response of data. So human response of data here we are returning that information. Then this is my another tool. So we are combining those tools in the list. We are binding them right and here is my entire chatbot. So this chatbot is nothing but it is llm with tools.invoke and it is returning that messages and we are creating this chatbot. We are adding additional condition along with the tool conditions and all. Right? So if I just go ahead and execute it and here we are applying the memory saver and finally this is the graph it looks like right. So start chatbot inside the tools there are two tools one is the tavly and one is the human assistance okay interrupt one right so interrupt one you can also see over here uh u if you see right this this uh this interrupt will happen in the tool node right in the tool node because the tools is having that human assistance now let's go ahead with the first question so first question over here is that user input says it is giving an input I need some expert guidance for building AI agents could you request assistance for me. Now this assistance will play a very important role, right? Because here we are providing a message and this message is matching to this particular dock string. So when LLM gets that message, it is going to call this specific tool instead of calling Tavly. Okay. So now let's go ahead and see here we are creating a thread ID. We are giving a user input which stream mode is equal to values each and everything and we are executing this. Okay. So here you can see a tool call is made. Initially it went to Tavly search. Okay, but it is not able to provide you the answer. Tavly search says that expert guidance for building AI agent. Now based on the result of the tool, I can see that it provides two relevant results, a blog post and a YouTube post. So what it has done is that uh for the first time when we call this particular function, it is calling the tably search API. So let's let's call this again. Okay, I need some expert uh guidance and assistance. I will change the message now. See what will happen for building AI agents. Could you please uh provide assistance to me? Okay. So now we are again executing. I need some expert guidance and assistance. You can see the tool call of human assistance has actually made. So this time when I just change the meth me message over here in the user input, I need some expert guidance assistance for building AI agent. You can see that a tool call is basically made. Okay. Now with respect to the human because now it has stopped over there. Now it is expecting human should provide some kind of input back right. So with respect to this you can see over here now human response we have provided this we the experts are here to help you out. We recommend you checking out langraph to build your agent. It's much more reliable and extensible than simple autonomous agent. So this is the message the human is basically giving. Now how do we go ahead and execute this message? We basically use this command. You remember in the top we we use this command and we are going to put this rumé is equal to data of human response. So whatever human response we are creating we are putting in this particular value and we are telling resume the flow of the execution and now when we go ahead and resume it here in graph.stream we give this particular human command and automatically you'll be able to see that the execution will happen. Now human we the experts we getting and then here you got the AI message. Thank you for recommendation. Langraph seems like a great tool for building AI agents. I'll make sure to keep that in mind to further assist. I'd like to ask a follow-up question. What specific these things and all. Now what you can do again now it has again interrupted right uh in sorry it is not interrupted now it is basically giving you as an AI message please let me know and I'll do my best to provide your tailored guidance assistance. Now what you can do is that again you can go ahead and put an interruption and again you can go ahead and provide a response. So when you are probably creating an end toend chatbot any number of time you can provide this kind of human feedback in the low right. So I hope you have understood this topic very much clearly. Hello guys. So in this video we are going to discuss about how you can build your own MCP servers. Along with that you'll also be seeing that how you can integrate any kind of MCB servers that you build along with your app. So here is one basic diagram. Here you can see there are three main components. One is MCP servers, MCP client and app. Whenever I talk about MCP servers here you can have multiple tools. Just imagine that there is a other company third party companies which are developing this kind of services. It can be simple mathematical uh you know calculations. It can be third party APIs, integrations, anything. It can be specifically written over here. uh here uh with respect to this MCP server it provides you context tools and prompts to the client and similarly you have something called as MCP client here the client maintains onetoone connection with the server inside the host app and finally you also have a app it can be a cloudy desktop or it can be any kind of app that you are specifically developing. So uh in this video what I am actually going to show you is that how we can go ahead and develop this entirely and how we can also build MCP server from basics or from scratch. Okay. So first of all what we are basically going to do is that we will be having this uh this application. Let's say that this is the application that I'm currently building. Okay. Inside this application we are going to use lang chain or lang graph. Okay. Application uh we will be having some kind of chatbot application in short. Okay. Now this chatbot application may have different different LLM integrated in this. So whenever a user provides any input okay so let's say a user provides any input. So based on this particular input, the LLM should be able to make a decision whether it has to make any kind of call from an MCP server. Okay. And let's say that this MCP server has some of the important tools. Let's say we have tools like addition, multiplication. I'm just showing this as an example. And let's say that we also go ahead and create one more tool here. um which is just like a weather call API. Okay, weather call API. Now here you'll be able to see that this is my MCP server itself and this MCP server is connected to this tools which are like add multiplication weather call APIs anything as such. So let's say if I go ahead and ask a question hey what is the weather of New York or Bangalore you know so the LLM obviously will not be able to answer because obviously LLM do not have live information so what this will do is that it will make a tool call and this time the tool call will be with the help of MCP protocol here internally there will be a client that will be developed which is called as MCP client okay and then once this communication is made then that specific uh you know API or tools whichever based on the input will be called and you finally get a response. Okay. So if I talk about like how this entire communication basically happens uh first of all when we get the input right direct the call will go to the MCP server. The MCP server will give you all the necessary tools along with uh what all information it has regarding that particular tool. Then the LLM will make a decision. uh then the LLM takes this particular input and passes it to the MCP server to get the response. So this is a basic kind of communication that actually happens and I have already covered in depth uh already in my MCP uh module itself right um uh in my previous videos. So this is how the basic communication basically happens right now here what we are going to focus on is that I will show you how you can go ahead and create your MCP server from scratch. Okay, here we are going to use one of the most popular library which is called as langchain and in langchain there is a library which is called as langchain adapters. Okay, so that we'll be going to use. Second, I will show you how you can go ahead and create your MCP client and whenever we talk about MCP protocol or whenever we talk about communication with the MCP servers, there are different different transport protocol that we use. Okay, transport protocol that we use. Now some of the transport protocol um like um there are some kind of arguments which actually helps uh you to communicate with any kind of tools itself. So one of the tool that we are going to use is something called as HTD IO and the other tool that we are basically going to use is uh related to HTTP protocol. Okay. So we'll try to understand what are the differences between them and uh we'll try to also use them. Uh again from coding point of view I'll show you how this also works. Okay, we will be developing our MCP server. We'll also be developing our MCP client in this MCP server. Uh when I talk with respect to the tools this tool, one of the tool we will try to run it with the help of transport protocol that is HTDIO and the other one we will try to use HTTP. Okay. And we'll also talk about the differences what exactly this both this transport mechanism uh how does it vi you know. So um now let me quickly go ahead and let me open and this we are going to completely start from scratch. So first of all I am inside my drive. Okay. So this is the MCP demo lang chin. So here you can see uh I will open my cursor ID. I hope everybody has the cursor ID now. Okay. Now from this cursor ID what I am actually going to do is that I'm going to go ahead and open this particular folder location as my project. Okay. So here I will go ahead and give this particular path and I will select the folder. Okay. Now the first step uh when you are specifically using cursor or whenever you work in any kind of projects it is good that you try to uh create a environment. Right now before creating an environment uh I need to initialize this particular workspace as a UV u uh with the help of the UV package. Okay. So if you know about UV uh it is quite faster. uh you'll be able to probably do the development very very much fast with respect to the package management of the entire project itself right uh any Python project so uh let's say that I'm going to go ahead and initialize this workspace with the help of UV package so I'll write uv in it so this is the first step now here you can see based on this there are some files that has been already created okay and uh if I talk with respect to all the specific files that we have created uh over here one very important thing is that U you have to go ahead and see which Python version this entire u you know the basic package is basically created with. So here you can see Python version is 3.13 here you have this pi project.2ml. So right now the dependency is empty because we have not installed any kind of dependencies right now right but we will go ahead and install it right and this is the basic project information now to start with any project I will go ahead and create my virtual environment. In order to create the virtual environment with the help of UV, it is very simple. So I'll go ahead and write uv VNV. Okay. Now here it shows that okay my VNV environment has got created. Now any packages that I install I have to install inside this. So first of all I will go ahead and activate my environment. In order to activate I will just go ahead and copy this command and paste it over here. Okay. So now we have activated my environment itself. Okay. Now this is done. Now the next step is that we go ahead and install some of the packages. Okay. Now we will see how to install the packages. But before that I will just go ahead and create my requirement.txt. requirement.txt. Okay. Now with respect to requirement.txt uh I will just go ahead and write what all libraries I will be requiring. Okay. So two libraries that I specifically want to use. one is langchain grock and then you also have something like lang chain adapters right so as I said uh we going to go ahead and use um some of the libraries that are available with respect to this that is langchen adapters and with the help of langin adapters you will definitely be able to use this MCP properties even in langchen okay so here you can see I'll write lang mcp adapters sorry it is mcp adapters along Along with this uh I'm also going to use one library which is called as fast MCP. Fast MCP. Okay. So here with respect to fast MCP you can actually see this what exactly this is. Okay. So let me just go ahead and search for fast MCP again. So if I talk about fast MCP here you can see it is the fast Pythonic. It is written something like Pythonic way to build MCP server and client. Okay. So we going to specifically use this. This is a very very easy way of creating MCP tools and all. So definitely I will show you step by step how you can basically use this fast MCP library and develop your entire MCP servers from scratch. Okay. Step by step we will go ahead and implement it. Now quickly uh here we are going to create three more important files. Okay. Now what all files needs to be created based on this uh that is what I'm going to discuss and understand based on the use cases right uh I have to I've already told you that I'm going to use one MCP server which has this add multiplication and we'll use the transport as studio and we'll create another MCP server which will be communicating to this tool that is called as weather call API and it will use this HTTP tool right uh transport mechanism okay transport mechanical mechanism basically means the communication between the client and the MCP server how it is basically going to happen. Okay. And uh so what we are basically going to do is that over here I will just go ahead and uh write all my packages that is specifically required. Okay. And uh along with this I will also go ahead and import MCP. Okay. So this MCP will actually help us to use the package fast MCP itself. Okay. Now here is my requirement.txt. The next step is that how do I go ahead and install all these particular libraries. It is very simple. I will go ahead and write uv add minus r requirement.txt like how we used to write pip install requirement.txt. Similarly we'll go ahead and do this. Okay. So now I'm going to go ahead and clear the screen and just to confirm whether all the installation has happened or not. So here you can basically go ahead and check out all the installation with respect to this. Okay. Uh till here everything looks good. uh our installation has happened perfectly and uh we have already uh you know installed all the packages that is required. Okay. Now uh let me just go ahead and create some important tools right with respect to the MCP server. So first tool that I'm actually going to create it's nothing but math server. Okay. So math server. py. So this is just like my MCP server. And here we are going to define some of the tools that we are basically going to use. Okay. So quickly in order to use this as I said I'm going to use fast MCP. So I'll write from MCP dots server dot fast MCP. I'm going to go ahead and import fast MCP. Okay. And once we do this uh the next step is that we need to initialize this MCP right. So I'll go ahead and write MCP is equal to fast MCP and I will give my tool name which is nothing but math. Okay. So I'll give my tool name which is nothing but math. Okay. Now uh inside this tool uh sorry inside the server uh this is just a server name okay not tool name uh because math is just a basic server name over here. Then the next step is that I will just go ahead and write add the rate MCP. And this is how we go ahead and create our first tool which is present inside this MCP server. So I'll create a definition. I'll write add. I'm just starting with a basic example so that see the limit as we say right? you want to go ahead and write create any kind of tool but it is un important that you understand from basic stuffs right so then my second parameter will be is equal to int and this I'm going to give return it in the form of integer here uh I'm going to probably provide some dock string and based on this dock string the llm will be able to understand which tool to specifically call so here I will write add two numbers okay and then we're going to go ahead and return a + b. Okay. Then the next tool is nothing but mcp. tool. And here we going to go ahead and define multiply a colon int, comm, b col, int. Again, you can go ahead and define any number of tools as you want. So this will return a int type. And here I'll just go ahead and write multiply multiply two numbers. Okay. some information that I'm specifically giving and I'll go and write return a return a multiplied by b. Okay. Now the thing is that see I am planning to create this mcp server with respect to this addition multiplication or any kind of tool on the transport hddio. Now we need to understand what this htdiod transport basically means. Okay. And uh what you will be able to do from it uh and it is important that we get a clear understanding about that because uh many people have seen that they try to write this particular code but they fail to explain this. Okay. Um what does mcp.tr run you know so let's say that I want to run this particular file. How do I go ahead and run this? First of all I'll go ahead and write the code. So quickly I will write mcp.run. So here what I'm actually going to do I'll just say if_ name double equal to main and here I will just go ahead and write mcp.trun and we're going to run this entire application of mcp using the transport transport double equal to stddio IO. Okay. Now here we have used a transport called as H std IO. Now we need to understand what this transport is and for this I will just go ahead and put some basic information so that you should be able to read it within the material itself. Okay. So here I will write two important comments. The transport is equal to H stdio. And here one more sentence. Okay, it tells the server to use standard input output to receive and respond to the tool functional calls. Now see what this is right when we say input output right the standard input output. Now standard input output is like let's say if this is a server if it is running this will specifically run in some kind of command prompt. Let's say in in in in one of the scenario what we can do is that if I have a client and I want that client to interact with this particular server then what we'll do if we have written this transport is equal to stdio we will run this particular file directly in the command prompt and get the input and output there itself like let's say if I want to probably get give an input that input should go with respect to the command line itself hit any function and get the response out there and the client should be able to read the information out directly from the uh HDI out that basically means from the command prompt itself. Right? So this kind of thing is very helpful if you really want to test out things locally. You have uh your server executed in the locally itself and you really want to go ahead and test it with the client. Right? So at that point of time you can use HTDIO. Okay. So this is the basic functionality with respect to this. So this is one of the server that we have basically created. The another server that I am really interested in creating is about uh let's say there may be a third party API call you know that API call can be with respect to weather it can be anything as such but just to show it to you I will quickly go ahead and create one weather py file okay now weatherp file see now at the end of the day I'll also talk about like how do you probably take it to the production and what exactly goes into the production also So I'll not show you directly by executing this in the cloud but I'll give you a brief idea how things works over here. Right? So here I will go ahead and write from MCP.Server dotfast MCP import fast MCP. Okay. And then I'm going to go ahead and write MCP is equal to fast MCP. And this time this particular server name will be my weather. Okay. Now here I'm going to go ahead and create my MCP tool. Okay. Now in a real world scenario if I talk about that this is my MCP server and I want to probably take an input and give the weather of a specific location. That is the code that I'm going to write it over here. Okay. But for right now I'll just going and defining something. So I'll go ahead and write hey this is my get weather functionality and let's say this is my location. Okay this is my location. This is my str and this will basically return a str. Okay. And then what I will do, I will go ahead and write my dock string. Get the get the get the weather location. Okay, weather location. Now, this can be any code. This can be a code which will be interacting with some kind of third party API and getting the weather. Right? For right now, I'll just return some constant value. So let's say here I'll write it's it's always rainy. It's always raining in California. Let's say I'll just go ahead and write this message. Okay, it may not be a true weather but I just want to give you an idea. Let's say that this is the output of my API that I'm getting here. You can write any code with respect to interacting with some kind of APIs. And then I will go ahead and write if underscore name main right. So here my program execution will basically start this should be double equal to okay now what I will do I will quickly write mcbp.tr run and this time I'm going to use another transport. See whenever I want something see the before the one one transport mechanism that we have specifically used is nothing but HDIO right HDIO I've told you the importance of it in this we are going to use streamable HTTP right HTTP now you need to understand what this exactly means okay so guys now let's understand what this transport streamable HTTP will do okay now here uh in order to make you understand what exactly the function that is right. So I'll just go ahead and open my terminal. Now inside my terminal, what I will do? I will just go ahead and run this. See python weather. py. Let's run this. Okay. Now here you can see that this entire application, this entire server is running in this particular URL. Okay. When we use streamable HTTP transport, what it is going to do is that it is going to run as an API service itself. Okay. Similarly, if I go ahead and run this math server right in HDDIO, it will not run like that. See here, it will not run like that. Instead, it will try to get it'll it it will not run in any kind of HTTP protocol, but instead it uses standard input and output. Okay. So, if I just go ahead and execute this Python math server. py here, you can see that nothing is happening, right? So, that basically means internally as in the command prompt it is getting executed. Okay? But if I see in this particular use case when we are using weather. py with the help of transport is equal to streamable http. Here you can see that it is working it is running as in the form of an API with this particular URL. So here after this transport you can also go ahead and set up your URL and all. And with respect to that you can also set up the port. Okay. But right now we are not running this. We are running this as an HTTP. Right. So by default you'll be able to see it is taking my local host and the default port is 8,000. Now the question rises Chris fine you you told me the differences between streamable HTTP and obviously HTD uh uh where my transport was HTT out right so here I have used HTIO right so you you have told the differences between this but how do we go ahead and integrate it from the client so here what I will do I'll go ahead and write client py so see I have created two servers one is the math server and one is the weather server now it's time that we go ahead and go go ahead and write our client py file so let this things get running. Now I'm going to go ahead and focus on understanding that how do you go ahead and write the client py. At the end of the day this client py should be able to interact with maths server py and weather. py. So for this I'll be using from langchin_mcp adapters doclient. So we have to first of all go ahead and create a client and this client should be according to the documentation that is given from the langraph it should be a multi-server MCP client. Okay, that basically means supports multiserver itself. Then in lang graph whenever I want to probably call any of this particular client, we need to create an agent. That agent will be responsible in integrating all these particular models, llm models or tools. Tools basically means all these MCP tools and all right. So for this we will be using pre-built. So from lang graph dot pre-built. So first of all I will just go ahead and quickly add lang graph also because I require lang graph. Okay. So here I'll open my command prompt another command prompt and I'll write hey uv add minus r requirement.txt. Okay. So this is perfect. And then you'll be able to see that if I just go back to my client. py now I will be able to import it. So from langu dot pre-built create react agent. So for creating an agent uh so that based on the input the agent the LLM will be able to act an agent itself. And uh you know in my previous videos I have all discussed about this uh if you're following the series of videos that we have developed right then from langchain grock import chat. So I'm going to go ahead and use chat gro also. And then from langchain open aai openai we will not going to use. So from langchain core I'm also going to go ahead and use or let's say for right now I will just go ahead and use like this from env import load_.env and then I'll go ahead and initialize this load_.env env and then I will also import a sync io right now the next thing is that I definitely require my env file so quickly let me go ahead and create myv file this is just for my llm model right so I'll write gro api key since I'm going to use grock API key now I hope everybody if you're following all the tutorials that I have created till now you should know how to create a gro API key right so here is my gro API key I'll go back to my client and inside this particular client I'll start uh going and writing my content right uh my code sorry now what I'm going to do I'll go ahead and write a sync definition main okay and here we are basically going to go ahead and create our client this client that we are going to create will be my multi-server HTTP client sorry MCP client and here I will give the client key value pairs right so the first client that I want to create so first server that I want to create right so this client will be able to interact act with this MCP server. So it will be my math server. In the math server, let's say the command that I want to use in order to execute my math server will be nothing but Python because you can use Python or UV. It is up to you. Okay, Python. And then the next parameter that we give is argument. Okay, let's see some there are a lot of suggestion that comes in this right. So arguments. So inside the arguments I will give my another parameter and that parameter will be nothing but it will be my file name. So here I'm going to go ahead and write maths server. py. Please make sure to give the right location. So here since this is my current working directory I'm directly giving the name of the file. If it is inside any folder I have to give the entire relative path. Okay. So once this is done sorry not relative path absolute path. So here I'll go ahead and write the comment ensure correct absolute path. Okay. Then my next parameter over here is nothing but my transport protocol. Sorry my transport uh metrics that we really want to give. So here based on the transport that we have used what transport we will be using it is nothing but stdio. Okay. So here I will go ahead and write stdio. Okay. So this actually does completes all our parameter with respect to maths. Now similarly I will go ahead and add my another tool. So this is my maths tool over here. Okay, I'll go ahead and write it like this. Now coming to the next tool, it is nothing but my weather tool. So if you see my weather tool, it'll be something like this. Weather localhost 8000/MCP ensure server is running here. So if you see over here my server, it is running where it is running in this local host. And when I do /m MCP that basically means it will be able to get all the MCP servers that it is running uh all all sorry this particular weather where it is specifically running in this particular URL right so here obviously my local host is there but if you see /mcp this is where we will be able to find the entire MCP running okay so this will be my URL over here right so now till here it is really really good easy itself here we have just created our multiserver client u now remember this clown client is what will be interacting with this particular servers. Right? So now I will go ahead and quickly write import OS and then I will go ahead and set up my environment. So OS environment it'll be nothing but grock API_key and here I will just write os.get env with my gro apiore key. Okay then I'll go ahead and write my tools. it will be await um first of all in order to get the tools I can use client dot get tools okay now see this client is nothing but this client right and when I write dot get tools I will be getting the information of both these tools like math and weather right then I will go ahead and initialize my model my model is equal to chat gro and I'm going to go ahead and use a model name which is nothing but quen qw 32 billion parameter Okay, then uh I will go ahead and create my agent and this agent will be create react agent and here I'm going to go ahead and write model, tools, right? So this is the two important parameter that we need to give in order to make the agent. Now I can use this agent and directly call uh invoke with respect to any messages that we specifically give. So let's say if I just go ahead and execute this math response. So here you can see I'm just executing this. Just a second. So import OS. This is done. Uh okay. Now math response await agent.invoke. I'm giving the messages equal to ro with user content. I've just written what is 3 * 5 * 2 okay 3 + 5 * 2 and here I should be able to print my response print my response so here in order to print my response I will write hey maths response colon okay math response is equal to I'll just go ahead and give this and then I'll write math response I will take the messages key I will take the last message that is available out there and I will go ahead and read the content. Okay, dot content will give the output of the maths response. Okay, now since this main function is a sync so in order to run this we are basically going to use a sync io rain. Okay. And here we are going to call the main function. Okay. So whenever we use async io uh whenever we define any method that is async, we have to go ahead and run this with this particular uh library which we have imported it over here. Okay. So here we are just trying to get the math response. Okay. Now let's go ahead and execute this. I will go ahead and open my command prompt. Now understand very important thing. When I'm calling this agent, right, it is invoking which tool? Based on this particular message, it will invoke this tool. And you know in this tool the transport is H stdiodio that basically means this tool is going to run in the normal standard IO device standard input output device that is nothing but command line. So that the input will directly go over there and get the output from there. Okay. So here in order to execute this if I go ahead and write python client.py okay now you should be able to see I'll cancel this. You should be able to see what will be the output for this. What's 3 + 5 * 12. Okay. So this is my input. Now you should be able to see what will be the house. Here's a step-by-step breakdown. Addition 3 + 5 is equal to 8 multiplication 8 * 2 is equal to 19. Math response is nothing but the result of 3 + 5 * 2 is 96. So 8 * 12 it is nothing but 96. This is absolutely perfectly fine. Okay. So here you can quickly see that how we are able to call our MCP uh server and that is nothing but our math server which is running in this HTDO right now the other thing is that if I also want to check the weather weather server right so for the weather server again I will go ahead and write like something like this see I'll give a question quickly and it will be the same thing see weather response await agent.invoke invoke content what is the weather in NYC or California right California now this it'll take this particular message but right now I have hardcoded the output it it always rain it always uh rains in it is always raining in California right we have written like this so my weather response should probably come the same thing what we are getting directly from the weather py okay so I will go ahead and run this once Okay. And before running this, I will also go ahead and print the output. Okay. So this is my weather response. Yeah, it is printed. So now if I just go ahead and execute this again, pythonclient.py. First of all, I should be getting my math response. And the second thing is that I should be getting my weather response. Okay. So quickly let's see this. And this is how you are basically communicating from one client to multiple servers itself. Right? So it is taking some amount of time. Okay. See at sometimes you know sometimes this kind of errors will come. You just need to go ahead and restart it. Okay. But now it'll not it will not uh this kind of error will not come. Okay. So now you'll be able to see that uh it'll do the execution. So here you can see the result of 3 + 5 weather response. The tool indicated it is always raining in California but in reality California has a diverse climate. So LLM is also able to add some information which is good. But here now the tool is basically returning this. Okay. So that is the reason uh again it depends on what kind of API functionality you're implementing it. The best part is that this is running in a streamable HTTP. So like it's running in the form of a in in some URL. You can just see that and we integrating that in client.py right and this is the URL that we getting it with /mcb right and all these things with the help of langchen adapter. Right? So I hope uh you are able to understand this particular example. Uh now what you can do is that you can close all the thing all the all the all the servers where what you're running but these are some some servers that are independently running and you're integrating them in a single client. Okay. So these were two ways of calling one is HDIO transport and streamable HTTP transport. So here we have created a client. So in short what all things we did? So we created a client and this client were able to communicate with two MCP servers. Okay. So this communication was basically happening this MCP server. This MCP server it is basically communicating with your transport equal to HTD IO and this MCP server you are able to communicate with HTTP protocol transport protocol and here see this entire thing is basically set up with MCP protocol itself. So we had that MCP server client right in this you had some tools like math addition subtraction whatever tool you want to create and this was like an weather API right the main thing is that when you're running this tool you are basically communicating with respect to the response from the HTD IO itself that basically means from the command prompt here we were using some kind of URL right so that is the reason we use HTTP so I hope uh you understood this particular video. I hope you understood the coding mechanism that we uh specifically did, how we implemented each and every step. Uh this was it from my side. I hope you like this particular video. I'll see you on the next video. Thank you. Thicker.\n"
     ]
    }
   ],
   "source": [
    "print(state['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73aec53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creatin a tool that takes transcripts as input and response the title and content of the video\n",
    "\n",
    "def transcript_to_title_and_content(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Uses Groq LLaMA3 and Mixtral to turn the transcript into:\n",
    "    - a good title\n",
    "    - well-structured content/summary\n",
    "\n",
    "    Saves:\n",
    "      - raw llama3_output\n",
    "      - raw mixtral_output\n",
    "      - final title + content into state.\n",
    "    \"\"\"\n",
    "    transcript = state.get(\"transcript\")\n",
    "\n",
    "    if not transcript:\n",
    "        state[\"final_output\"] = \"No transcript available to generate title and content.\"\n",
    "        state[\"title\"] = None\n",
    "        state[\"content\"] = None\n",
    "        return state\n",
    "\n",
    "    # 1) First pass: LLaMA3 creates initial title + content\n",
    "    system_prompt_llama = (\n",
    "        \"You are an expert at summarizing educational YouTube lectures. \"\n",
    "        \"Given a full transcript, you must generate:\\n\"\n",
    "        \"1) A concise, catchy title (max ~12 words).\\n\"\n",
    "        \"2) A detailed, well-structured content/notes section.\\n\\n\"\n",
    "        \"Return STRICTLY in this format:\\n\\n\"\n",
    "        \"TITLE: <your title here>\\n\\n\"\n",
    "        \"CONTENT:\\n<your multi-paragraph content here>\\n\"\n",
    "    )\n",
    "\n",
    "    llama_resp = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt_llama},\n",
    "            {\"role\": \"user\", \"content\": transcript},\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    llama_out = llama_resp.choices[0].message.content\n",
    "    state[\"llama3_output\"] = llama_out\n",
    "\n",
    "    # 2) Second pass: Mixtral refines and improves LLaMA3 output\n",
    "    system_prompt_mixtral = (\n",
    "        \"You are a careful editor. You will receive a draft TITLE and CONTENT. \"\n",
    "        \"Improve clarity, structure, and flow, but keep the meaning.\\n\\n\"\n",
    "        \"Return in EXACTLY this format:\\n\\n\"\n",
    "        \"TITLE: <improved title>\\n\\n\"\n",
    "        \"CONTENT:\\n<improved multi-paragraph content>\\n\"\n",
    "    )\n",
    "\n",
    "    mixtral_resp = groq_client.chat.completions.create(\n",
    "        model=\"mixtral-8x7b-32768\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt_mixtral},\n",
    "            {\"role\": \"user\", \"content\": llama_out},\n",
    "        ],\n",
    "        temperature=0.4,\n",
    "    )\n",
    "    mixtral_out = mixtral_resp.choices[0].message.content\n",
    "    state[\"mixtral_output\"] = mixtral_out\n",
    "\n",
    "    # 3) Parse Mixtral output into title + content\n",
    "    title = None\n",
    "    content = None\n",
    "\n",
    "    if \"CONTENT:\" in mixtral_out:\n",
    "        head, body = mixtral_out.split(\"CONTENT:\", 1)\n",
    "        # remove \"TITLE:\" prefix if present\n",
    "        title = head.replace(\"TITLE:\", \"\").strip().strip(\":\")\n",
    "        content = body.strip()\n",
    "    else:\n",
    "        # fallback: use full text as content, first line as title\n",
    "        lines = mixtral_out.strip().splitlines()\n",
    "        title = lines[0].strip() if lines else \"Generated Title\"\n",
    "        content = mixtral_out\n",
    "\n",
    "    # 4) Save final values in state\n",
    "    state[\"title\"] = title\n",
    "    state[\"content\"] = content\n",
    "    state[\"final_output\"] = content  # if you want this as main response\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5e482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
